{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uI7qB4jtoe7",
        "outputId": "7aed7beb-b51d-4ea5-cf81-e2f3931b4e4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
            "/home/physics/phuftc/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU Device: NVIDIA RTX A4000\n",
            "Number of GPUs: 1\n",
            "Remember to use cuda device from here on\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import Subset, Dataset, DataLoader, IterableDataset, TensorDataset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torch.amp import autocast\n",
        "import math\n",
        "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
        "# req torch, torchvision, einops, tqdm, ema_pytorch, accelerate\n",
        "from IPython.display import display\n",
        "from einops import rearrange, reduce, repeat\n",
        "import glob\n",
        "from ema_pytorch import EMA\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from accelerate import Accelerator\n",
        "from pathlib import Path\n",
        "from random import random\n",
        "from functools import partial\n",
        "from collections import namedtuple\n",
        "\n",
        "import os\n",
        "CWD = os.getcwd()\n",
        "\n",
        "# Device stuff\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Remember to use {device} device from here on\")\n",
        "print(os.chdir(\"../\"))\n",
        "# %cd /home/physics/phuqza/E9/DDPM-HL-LHC/\n",
        "from DDPMLHC.config import *\n",
        "from DDPMLHC.calculate_quantities import *\n",
        "from DDPMLHC.data_loading import *\n",
        "from DDPMLHC.generate_plots.overlaid_1d import create_overlay_plots\n",
        "from DDPMLHC.generate_plots.bmap import save_to_bmap\n",
        "\n",
        "\n",
        "# Some functions from denoising_diffusion_pytorch that are required but couldn't import\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YPzvhe08tied"
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(tensor_images, scale_factor=8):\n",
        "    to_pil = T.ToPILImage()\n",
        "    pil_images = [to_pil(image) for image in tensor_images]\n",
        "\n",
        "    for img in pil_images:\n",
        "        # Upscale the image\n",
        "        upscaled_img = img.resize(\n",
        "            (img.width * scale_factor, img.height * scale_factor), \n",
        "            Image.NEAREST  # or Image.BOX for smoother results\n",
        "        )\n",
        "        display(upscaled_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load last checkpoint and train the model\n",
        "# def load_and_train(\n",
        "#     diffusion,\n",
        "#     dataloader,\n",
        "#     num_epochs,\n",
        "#     device,\n",
        "#     save_dir,\n",
        "#     lr=1e-4\n",
        "# ):\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "#     # Get last epoch number\n",
        "#     checkpoint_files = glob.glob(os.path.join(save_dir, 'checkpoint_epoch_*.pth'))\n",
        "#     last_epoch = 0\n",
        "#     if checkpoint_files:\n",
        "#         epoch_numbers = []\n",
        "#         for f in checkpoint_files:\n",
        "#             try:\n",
        "#                 epoch_num = int(f.split('epoch_')[1].split('_loss')[0])\n",
        "#                 epoch_numbers.append(epoch_num)\n",
        "#             except:\n",
        "#                 continue\n",
        "#         last_epoch = max(epoch_numbers) if epoch_numbers else 0\n",
        "\n",
        "#     # Load checkpoint if exists\n",
        "#     if last_epoch > 0:\n",
        "#         checkpoint_pattern = os.path.join(save_dir, f'checkpoint_epoch_{last_epoch}_*.pth')\n",
        "#         checkpoint_file = glob.glob(checkpoint_pattern)[0]\n",
        "#         print(f\"Loading checkpoint: {checkpoint_file}\")\n",
        "#         checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "#         diffusion.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     else:\n",
        "#         print(\"Starting fresh training\")\n",
        "\n",
        "#     optimizer = optim.Adam(diffusion.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "#     # batch = len(dataloader)\n",
        "#     # for epoch in range(last_epoch, last_epoch + num_epochs):\n",
        "#     #     running_loss = 0.\n",
        "#     #     last_loss = 0.\n",
        "#     #     print(f\"\\nEpoch {epoch + 1}/{last_epoch + num_epochs}\")\n",
        "#         # diffusion.train(True)\n",
        "#         # for i,batch_data in enumerate(dataloader):\n",
        "#         #     batch_data = batch_data.to(device)\n",
        "#         #     optimizer.zero_grad()\n",
        "#         #     loss = diffusion()  # Ensure targets are correctly defined and batched\n",
        "#         #     loss.backward()\n",
        "#         #     optimizer.step()\n",
        "#         #     running_loss += loss.item()\n",
        "\n",
        "#         #     print(f'Train Loss: {loss.item():.4f}')\n",
        "#         #     if i % 100 == 0:\n",
        "#         #         last_loss = running_loss / 1 # loss per batch\n",
        "#         #         print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "#         #         tb_x = epoch_index * len(training_loader) + i + 1\n",
        "#         #         tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "#         #         running_loss = 0.\n",
        "\n",
        "#         # checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{last_loss:.4f}.pth')\n",
        "#         # torch.save({\n",
        "#         #     'epoch': epoch,\n",
        "#         #     'model_state_dict': diffusion.state_dict(),\n",
        "#         #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         #     'loss': last_loss,\n",
        "#         # }, checkpoint_path)\n",
        "#         # print(f'Checkpoint saved: {checkpoint_path}')\n",
        "#     progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     loss_array = []\n",
        "#     for i, images in progress_bar:\n",
        "#         images = images.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = diffusion(images)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         loss_array.append(loss)\n",
        "#         running_loss += loss.item()\n",
        "#         # print(running_loss)\n",
        "#         avg_loss = running_loss / (i + 1)\n",
        "#         # avg_loss = np.mean(loss_array)\n",
        "#         progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}'})\n",
        "\n",
        "#     # Save checkpoint at the end of each epoch\n",
        "#     checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{avg_loss:.4f}.pth')\n",
        "#     torch.save({\n",
        "#         'epoch': epoch,\n",
        "#         'model_state_dict': diffusion.state_dict(),\n",
        "#         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         'loss': avg_loss,\n",
        "#     }, checkpoint_path)\n",
        "#     print(f'Checkpoint saved: {checkpoint_path}')\n",
        "\n",
        "\n",
        "## Custom reimplementation of Trainer from DDPM\n",
        "## Avoid subclassing because we do not want to pass in literal files\n",
        "\n",
        "class PUTrainer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        dataloader,\n",
        "        train_batch_size = 100,\n",
        "        gradient_accumulate_every = 1,\n",
        "        augment_horizontal_flip = True,\n",
        "        train_lr = 1e-4,\n",
        "        train_num_steps = 200,\n",
        "        ema_update_every = 10,\n",
        "        ema_decay = 0.995,\n",
        "        adam_betas = (0.9, 0.99),\n",
        "        save_and_sample_every = 1000,\n",
        "        num_samples = 25,\n",
        "        results_folder = './ML/results',\n",
        "        amp = False,\n",
        "        mixed_precision_type = 'fp32',\n",
        "        split_batches = True,\n",
        "        convert_image_to = None,\n",
        "        calculate_fid = True,\n",
        "        inception_block_idx = 2048,\n",
        "        max_grad_norm = 1.,\n",
        "        num_fid_samples = 50000,\n",
        "        save_best_and_latest_only = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # accelerator\n",
        "\n",
        "        self.accelerator = Accelerator(\n",
        "            split_batches = split_batches,\n",
        "            mixed_precision = mixed_precision_type if amp else 'no'\n",
        "        )\n",
        "\n",
        "        # model\n",
        "\n",
        "        self.model = diffusion_model\n",
        "        self.channels = diffusion_model.channels\n",
        "        # is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
        "\n",
        "        # default convert_image_to depending on channels\n",
        "\n",
        "        # if not exists(convert_image_to):\n",
        "        #     convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)\n",
        "\n",
        "        # sampling and training hyperparameters\n",
        "\n",
        "        # assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
        "        self.num_samples = num_samples\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
        "\n",
        "        self.train_num_steps = train_num_steps\n",
        "        self.image_size = diffusion_model.image_size\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        # dataset and dataloader\n",
        "\n",
        "        # self.ds = Dataset(folder, self.image_size, augment_horizontal_flip = augment_horizontal_flip, convert_image_to = convert_image_to)\n",
        "        # self.ds = dataset\n",
        "        # # assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n",
        "\n",
        "        # dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n",
        "        dl = dataloader\n",
        "        print(dl)\n",
        "\n",
        "        dl = self.accelerator.prepare(dl)\n",
        "        print(len(dl))\n",
        "        self.dl = self.cycle(dl=dl)\n",
        "\n",
        "        # optimizer\n",
        "\n",
        "        self.opt = optim.Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
        "\n",
        "        # for logging results in a folder periodically\n",
        "\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
        "            # print(\"EMA model\", self.ema.ema_model)\n",
        "            self.ema.to(self.device)\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok = True)\n",
        "\n",
        "        # step counter state\n",
        "\n",
        "        self.step = 0\n",
        "\n",
        "        # prepare model, dataloader, optimizer with accelerator\n",
        "\n",
        "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
        "\n",
        "        # FID-score computation\n",
        "\n",
        "        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n",
        "\n",
        "        # if self.calculate_fid:\n",
        "        #     from denoising_diffusion_pytorch.fid_evaluation import FIDEvaluation\n",
        "\n",
        "        #     if not is_ddim_sampling:\n",
        "        #         self.accelerator.print(\n",
        "        #             \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n",
        "        #             \"Consider using DDIM sampling to save time.\"\n",
        "        #         )\n",
        "\n",
        "        #     self.fid_scorer = FIDEvaluation(\n",
        "        #         batch_size=self.batch_size,\n",
        "        #         dl=self.dl,\n",
        "        #         sampler=self.ema.ema_model,\n",
        "        #         channels=self.channels,\n",
        "        #         accelerator=self.accelerator,\n",
        "        #         stats_dir=results_folder,\n",
        "        #         device=self.device,\n",
        "        #         num_fid_samples=num_fid_samples,\n",
        "        #         inception_block_idx=inception_block_idx\n",
        "        #     )\n",
        "\n",
        "        if save_best_and_latest_only:\n",
        "            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n",
        "            self.best_fid = 1e10 # infinite\n",
        "\n",
        "        self.save_best_and_latest_only = save_best_and_latest_only\n",
        "    def cycle(self, dl):\n",
        "        while True:\n",
        "            for data in dl:\n",
        "                yield data\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.accelerator.device\n",
        "\n",
        "    def save(self, i):\n",
        "        if not self.accelerator.is_local_main_process:\n",
        "            return\n",
        "\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.accelerator.get_state_dict(self.model),\n",
        "            'opt': self.opt.state_dict(),\n",
        "            'ema': self.ema.state_dict(),\n",
        "            'scaler': self.accelerator.scaler.state_dict() if self.accelerator.scaler is not None else None,\n",
        "        }\n",
        "\n",
        "        torch.save(data, str(self.results_folder / f'model-step{i}.pth'))\n",
        "\n",
        "    def load(self, milestone):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        data = torch.load(str(self.results_folder / f'model-{milestone}.pth'), map_location=device, weights_only=True)\n",
        "\n",
        "        model = self.accelerator.unwrap_model(self.model)\n",
        "        model.load_state_dict(data['model'])\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.opt.load_state_dict(data['opt'])\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema.load_state_dict(data[\"ema\"])\n",
        "\n",
        "        if 'version' in data:\n",
        "            print(f\"loading from version {data['version']}\")\n",
        "\n",
        "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
        "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
        "\n",
        "    def num_to_groups(self,num, divisor):\n",
        "        groups = num // divisor\n",
        "        remainder = num % divisor\n",
        "        arr = [divisor] * groups\n",
        "        if remainder > 0:\n",
        "            arr.append(remainder)\n",
        "        return arr\n",
        "    def train(self):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
        "\n",
        "            while self.step < self.train_num_steps:\n",
        "                self.model.train()\n",
        "\n",
        "                total_loss = 0.\n",
        "\n",
        "                for _ in range(self.gradient_accumulate_every):\n",
        "                    data = next(self.dl).to(device)\n",
        "\n",
        "                    with self.accelerator.autocast():\n",
        "                        loss = self.model(data)\n",
        "                        loss = loss / self.gradient_accumulate_every\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "                self.opt.step()\n",
        "                self.opt.zero_grad()\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "\n",
        "                self.step += 1\n",
        "                if accelerator.is_main_process:\n",
        "                    self.ema.update()\n",
        "                    divisible_by = (self.step % self.save_and_sample_every) == 0\n",
        "                    # print(\"???\")\n",
        "                    if self.step != 0 and divisible_by:\n",
        "                        self.ema.ema_model.eval()\n",
        "\n",
        "                        with torch.inference_mode():\n",
        "                            milestone = self.step // self.save_and_sample_every\n",
        "                            batches = self.num_to_groups(self.num_samples, self.batch_size)\n",
        "                            print(f\"right before all_images_list, step {self.step}\")\n",
        "                            # print(\"tt\", self.ema.ema_model.jetNG)\n",
        "                            # all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
        "                            all_images_list = self.ema.ema_model.sample(batch_size=4)\n",
        "\n",
        "                        # all_images = torch.cat(all_images_list, dim = 0)\n",
        "\n",
        "                        # utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
        "\n",
        "                        # whether to calculate fid\n",
        "\n",
        "                        # if self.calculate_fid:\n",
        "                        #     fid_score = self.fid_scorer.fid_score()\n",
        "                        #     accelerator.print(f'fid_score: {fid_score}')\n",
        "\n",
        "                        # if self.save_best_and_latest_only:\n",
        "                        #     if self.best_fid > fid_score:\n",
        "                        #         self.best_fid = fid_score\n",
        "                        #         self.save(\"best\")\n",
        "                        #     self.save(\"latest\")\n",
        "                        # else:\n",
        "                        if self.step % 10 == 0:\n",
        "                            self.save(milestone)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        accelerator.print('training complete')\n",
        "\n",
        "\n",
        "################################\n",
        "def load_and_train(\n",
        "    diffusion,\n",
        "    dataloader,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    save_dir,\n",
        "    lr=1e-4\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Get last epoch number\n",
        "    checkpoint_files = glob.glob(os.path.join(save_dir, 'checkpoint_epoch_*.pth'))\n",
        "    last_epoch = 0\n",
        "    if checkpoint_files:\n",
        "        epoch_numbers = []\n",
        "        for f in checkpoint_files:\n",
        "            try:\n",
        "                epoch_num = int(f.split('epoch_')[1].split('_loss')[0])\n",
        "                epoch_numbers.append(epoch_num)\n",
        "            except:\n",
        "                continue\n",
        "        last_epoch = max(epoch_numbers) if epoch_numbers else 0\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if last_epoch > 0:\n",
        "        checkpoint_pattern = os.path.join(save_dir, f'checkpoint_epoch_{last_epoch}_*.pth')\n",
        "        checkpoint_file = glob.glob(checkpoint_pattern)[0]\n",
        "        print(f\"Loading checkpoint: {checkpoint_file}\")\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "        diffusion.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"Starting fresh training\")\n",
        "\n",
        "    optimizer = optim.Adam(diffusion.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(last_epoch, last_epoch + num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{last_epoch + num_epochs}\")\n",
        "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, images in progress_bar:\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = diffusion(images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            avg_loss = running_loss / (i + 1)\n",
        "            progress_bar.set_postfix({'Loss': f'{avg_loss:.7f}'})\n",
        "\n",
        "        # Save checkpoint at the end of each epoch\n",
        "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{avg_loss:.7f}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': diffusion.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f'Checkpoint saved: {checkpoint_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 :: Loading original data\n",
            "FINISHED loading data\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MAX_DATA_ROWS = None\n",
        "\n",
        "# === Read in data\n",
        "print(\"0 :: Loading original data\")\n",
        "tt = np.genfromtxt(\n",
        "    TT_PATH, delimiter=\",\", encoding=\"utf-8\", skip_header=1, max_rows=MAX_DATA_ROWS\n",
        ")\n",
        "pu = np.genfromtxt(\n",
        "    PILEUP_PATH, delimiter=\",\", encoding=\"utf-8\", skip_header=1, max_rows=MAX_DATA_ROWS\n",
        ")\n",
        "tt = EventSelector(tt)\n",
        "pu = EventSelector(pu)\n",
        "print(\"FINISHED loading data\\n\")\n",
        "bins=16\n",
        "# Ground truth ttbar jets\n",
        "NG_jet = NoisyGenerator(TTselector=tt, PUselector=pu, bins=bins, mu=0)\n",
        "# Second one to randomly generate and return pile-up events ONLY\n",
        "## Will use np.random.randint to generate NoisyGenerator.mu and then call next\n",
        "NG_pu = NoisyGenerator(TTselector=tt, PUselector=pu, bins=bins, mu=0, pu_only=True)\n",
        "\n",
        "# class OutData():\n",
        "#     def __init__(self, vector, axis=(0,0)):\n",
        "#         pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NGenForDataloader(Dataset):\n",
        "    def __init__(self, noisy_generator, njets=100):\n",
        "        self.ng = noisy_generator\n",
        "        self.jets = []\n",
        "        self.njets = njets\n",
        "        # next(self.ng)\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.ng._max_TT_no - 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        self.ng.select_jet(idx)\n",
        "        x = torch.from_numpy( self.ng.get_grid() )\n",
        "        # x = x.unsqueeze(0)\n",
        "        x = x.unsqueeze(0)\n",
        "        # self.jets.append(x)\n",
        "        # print(\"x jet img\", x.shape)\n",
        "        # y = x\n",
        "        return x\n",
        "\n",
        "# class NGenForDataloader(IterableDataset):\n",
        "#     def __init__(self, noisy_generator, batch_size=1, nsteps=1000):\n",
        "#         super().__init__()\n",
        "#         self.ng = noisy_generator # NoisyGenerator instance that feeds in jet data\n",
        "#         self.batch_size = batch_size # batch_size should select batch_size number of jets\n",
        "#         self.ng.reset()\n",
        "#         self.nsteps = nsteps\n",
        "#         self.current_jetno = self.ng._next_jetID\n",
        "#         # next(self.ng)\n",
        "    \n",
        "#     # USING THIS TO MAKE IterableDataset for model\n",
        "#     # So dataset is \"streamed in\" continously instead of specifically \"selecting\" a jet\n",
        "#     # Hence in each timestep, \n",
        "#     def generate_jet(self):\n",
        "         \n",
        "#         #     if :\n",
        "#         #         raise RuntimeError(\"Requested jet not in loaded set. Did nothing.\")\n",
        "#         #     # Define empty image tensor\n",
        "#         # while True:\n",
        "#         image_data = []\n",
        "#         # images = torch.empty(size=(self.batch_size,1,self.ng.grid_side_bins,self.ng.grid_side_bins))\n",
        "#         # Ensure that if the remainder of data cannot fit in batch_size, just fit thhe rest of the data into one batch\n",
        "#         jet_range = range(self.ng._next_jetID, self.ng._next_jetID + batch_size) if self.ng._max_TT_no - self.ng._next_jetID > batch_size else range(self.ng._next_jetID, self.ng._max_TT_no)\n",
        "#         for jet_no in jet_range:\n",
        "#             self.ng.select_jet(jet_no) # Select jet, now self.ng.current_event is set\n",
        "#             image = torch.from_numpy(self.ng.get_grid())# Converted to bins x bins image and now tensor\n",
        "#             image_data.append(image)\n",
        "#         # Now image_data is (self.batch_size, bins, bins)\n",
        "#         image_data = torch.stack(image_data)\n",
        "#         image_data = image_data.unsqueeze(1) # Add channels, set to 1\n",
        "#         # image_data = image_data.to(device)\n",
        "#         self.current_jetno = self.ng._next_jetID\n",
        "#         # print(\"image shape 0:::\", image_data[0,:,:,:])\n",
        "#         return image_data\n",
        "            \n",
        "#     def __iter__(self):\n",
        "#         while self.current_jetno < self.ng._max_TT_no:\n",
        "#         # if \n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#             yield self.generate_jet()\n",
        "#             if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#                 self.ng.reset()\n",
        "#                 self.current_jetno = 0\n",
        "#         # return self\n",
        "#                 print(\"reset\")\n",
        "        \n",
        "        \n",
        "#     def __next__(self):\n",
        "#         # if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#         #     raise StopIteration  # Stops for loop after\n",
        "#         # else: \n",
        "#         while self.current_jetno < self.ng._max_TT_no:\n",
        "#         # if \n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#             yield self.generate_jet()\n",
        "#             if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#                 self.ng.reset()\n",
        "#                 self.current_jetno = 0\n",
        "#         # raise StopIteration  # Stops for loop after\n",
        "#             # return self.generate_jet()\n",
        "#     def __len__(self):\n",
        "#         return self.nsteps\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         self.ng.select_jet(idx)\n",
        "#         x = torch.from_numpy( self.ng.get_grid() )\n",
        "#         # x = x.unsqueeze(0)\n",
        "#         x = x.unsqueeze(0)\n",
        "#         # y = x\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "51c517885cfa4c5db1b350775e481a4c",
            "9d9a21a1d7ac4c1082d537a32ab78669",
            "f81e3ba126724d7f95a5140b89c127a6",
            "64899dafc9b748dd985dd1800f5a18c9",
            "677cf3826fe5477c9c13f2458ae18788",
            "abc465482e8443c495f2156aa5fc4ceb",
            "4fc3db99c8434e708ab613a8de71da86",
            "c4c041868d084fd2805330058b588fa6",
            "dfcebb41e86c4cfb993522f80b7184ba",
            "cc34e955ddb9409db13400a9fe1d4af0",
            "fd30649a40ff4132b3365373e551173a"
          ]
        },
        "id": "8-L7djUbJRvp",
        "outputId": "3f6a255f-66c5-4f50-8f62-27ac575d10d3"
      },
      "outputs": [],
      "source": [
        "model = Unet(\n",
        "    dim=64,                  # Base dimensionality of feature maps\n",
        "    dim_mults=(1, 2, 4, 8),  # Multipliers for feature dimensions at each level\n",
        "    channels=1,              # E.g. 3 for RGB\n",
        ").to(device)\n",
        "\n",
        "class PUDiffusion(GaussianDiffusion):\n",
        "    def __init__(self, model, image_size, puNG: NoisyGenerator, jet_ng: NoisyGenerator,**kwargs):\n",
        "        super(PUDiffusion, self).__init__(model=model, image_size=image_size, **kwargs)\n",
        "        self.puNG = puNG\n",
        "        self.jetNG = jet_ng\n",
        "        self.channels = model.channels\n",
        "        self.mu_counter = 1\n",
        "    def cond_noise(self, x_shape, noise):\n",
        "        return self.pu_to_tensor(x_shape) if noise is None else noise\n",
        "        # return torch.zeros_like(x_start) if noise is None else noise\n",
        "    def generate_data(self, shape, NG: NoisyGenerator):\n",
        "        \"\"\"\n",
        "        This function generates image data matched to the correct shape\n",
        "        \"\"\"\n",
        "        # Start next jet\n",
        "        next(NG)\n",
        "        selected = NG.get_grid()\n",
        "        # If empty pile-up, return array of 0s instead since model should account for this\n",
        "        if selected.size == 0:\n",
        "            return  \"Error in PUDiffusion.generate_jet\"\n",
        "        # print(selected_pu.shape)\n",
        "        pu_tensor = torch.from_numpy(selected)\n",
        "\n",
        "        pu_tensor = torch.unsqueeze(pu_tensor,0)\n",
        "        # This tensor has dimensions BxCxHxW to match x_start\n",
        "        pu_tensor = torch.unsqueeze(pu_tensor,0)\n",
        "        pu_tensor = pu_tensor.expand(shape[0], shape[1], -1, -1) \n",
        "        pu_tensor = torch.zeros(shape)\n",
        "        pu_tensor = pu_tensor.to(self.device)\n",
        "        return pu_tensor\n",
        "    def pu_to_tensor(self, shape):\n",
        "        # Select random number of pile-ups (mu) to generate, max 200 for now since HL-LHC expected to do up to this\n",
        "        # We are doing it per batch\n",
        "        # if isinstance(t, int):\n",
        "        #     mu = np.random.randint(low=1, high=200, size=None)\n",
        "        # else:\n",
        "        #     mu = np.random.randint(low=1, high=200, size=None)\n",
        "        mu = 1\n",
        "        # print(mu)\n",
        "        NG = self.puNG\n",
        "        NG.mu = mu\n",
        "        NG.reset()\n",
        "        # next(self.puNG)\n",
        "        pu_tensor = self.generate_data(shape=shape, NG=NG)\n",
        "        return pu_tensor\n",
        "    def jet_to_tensor(self, shape):\n",
        "        NG = self.jetNG\n",
        "        # next(NG)\n",
        "        pu_tensor = self.generate_data(shape=shape, NG=self.jetNG)\n",
        "        return pu_tensor\n",
        "    # TODO: ddim_sample???\n",
        "    @torch.inference_mode()\n",
        "    def p_sample(self, x, t: int, x_self_cond = None):\n",
        "        b, *_, device = *x.shape, self.device\n",
        "        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n",
        "        # print(\"batched times\", t)\n",
        "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
        "        ######## MODIFY\n",
        "        noise = self.pu_to_tensor(x.shape) if t > 0 else 0 # no noise if t == 0\n",
        "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
        "        return pred_img, x_start\n",
        "    @autocast('cuda', enabled = False)\n",
        "    def q_sample(self, x_start, t, noise = None):\n",
        "        noise = self.cond_noise(x_shape=x_start.shape, noise=noise)\n",
        "\n",
        "        if self.immiscible:\n",
        "            assign = self.noise_assignment(x_start, noise)\n",
        "            noise = noise[assign]\n",
        "        # print(\"q_sample t\", t)\n",
        "        return (\n",
        "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
        "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
        "        )\n",
        "    @torch.inference_mode()\n",
        "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
        "        # print(\"p sample loop\")\n",
        "        # print(shape)\n",
        "        batch, device = shape[0], self.device\n",
        "\n",
        "        # img = torch.randn(shape, device = device)\n",
        "        # This function is called in PUTrainer to sample jets\n",
        "        # img = self.\n",
        "        # img = torch.zeros(shape)\n",
        "        img = self.jet_to_tensor(shape=shape) # Geenerates a jet\n",
        "        # img = img.to(self.device)\n",
        "        imgs = [img]\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
        "            self_cond = x_start if self.self_condition else None\n",
        "            img, x_start = self.p_sample(img, t, self_cond)\n",
        "            self.mu_counter +=1\n",
        "            imgs.append(img)\n",
        "\n",
        "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "        ret = self.unnormalize(ret)\n",
        "        return ret\n",
        "    @torch.inference_mode()\n",
        "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
        "        (h, w), channels = self.image_size, self.channels\n",
        "        # sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
        "        sample_fn = self.p_sample_loop\n",
        "        return sample_fn((batch_size, channels, h, w), return_all_timesteps = return_all_timesteps)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
        "        # print(\"is ddim\")\n",
        "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
        "\n",
        "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
        "        times = list(reversed(times.int().tolist()))\n",
        "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
        "\n",
        "        img = torch.randn(shape, device = device)\n",
        "        # img = self.cond_noise(x_start, sampling_timesteps)\n",
        "        imgs = [img]\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
        "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
        "            self_cond = x_start if self.self_condition else None\n",
        "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n",
        "\n",
        "            if time_next < 0:\n",
        "                img = x_start\n",
        "                imgs.append(img)\n",
        "                continue\n",
        "\n",
        "            alpha = self.alphas_cumprod[time]\n",
        "            alpha_next = self.alphas_cumprod[time_next]\n",
        "\n",
        "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
        "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
        "\n",
        "            # noise = torch.randn_like(img)\n",
        "            # noise = self.cond_noise(self_cond, noi)\n",
        "            noise = self.pu_to_tensor(x_start.shape)\n",
        "\n",
        "            img = x_start * alpha_next.sqrt() + \\\n",
        "                  c * pred_noise + \\\n",
        "                  sigma * noise\n",
        "\n",
        "            imgs.append(img)\n",
        "\n",
        "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "        ret = self.unnormalize(ret)\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):\n",
        "        b, c, h, w = x_start.shape\n",
        "        # print(x_start.shape)\n",
        "        # Select one pile-up at a time for each timestep $t$\n",
        "        # single_pileup = self.pu.select_event(np.random.randint(low=0, high=self.pu.max_ID, size=1))\n",
        "        # print(\"p_losses t\", t)\n",
        "\n",
        "        # noise = self.cond_noise(x_start.shape, noise=noise)\n",
        "        noise = torch.zeros_like(x_start)\n",
        "\n",
        "        # if noise is None: \n",
        "        # noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        # offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n",
        "\n",
        "        # if offset_noise_strength > 0:\n",
        "        #     offset_noise = torch.randn(x_start.shape[:2], device = self.device)\n",
        "        #     noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
        "\n",
        "        # noise sample\n",
        "\n",
        "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
        "\n",
        "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
        "        # and condition with unet with that\n",
        "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
        "\n",
        "        x_self_cond = None\n",
        "        if self.self_condition and random() < 0.5:\n",
        "            with torch.no_grad():\n",
        "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
        "                x_self_cond.detach_()\n",
        "\n",
        "        # predict and take gradient step\n",
        "\n",
        "        model_out = self.model(x, t, x_self_cond)\n",
        "\n",
        "        if self.objective == 'pred_noise':\n",
        "            target = noise\n",
        "        elif self.objective == 'pred_x0':\n",
        "            target = x_start\n",
        "        elif self.objective == 'pred_v':\n",
        "            v = self.predict_v(x_start, t, noise)\n",
        "            target = v\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {self.objective}')\n",
        "\n",
        "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
        "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
        "\n",
        "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
        "        return loss.mean()\n",
        "    def forward(self, img, *args, **kwargs):\n",
        "        # img = img.squeeze(0)\n",
        "        # print(\"???\", *img.shape)\n",
        "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
        "        assert h == img_size[0] and w == img_size[1], f'height and width of image must be {img_size}'\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        return self.p_losses(img, t, *args, **kwargs)\n",
        "\n",
        "\n",
        "# diffusion = GaussianDiffusion(\n",
        "#     model = model,\n",
        "#     image_size = 16,  # Size of your images (ensure your images are square)\n",
        "#     timesteps = 1000,  # Number of diffusion steps\n",
        "#     objective = \"pred_x0\",\n",
        "# ).to(device)\n",
        "\n",
        "diffusion = PUDiffusion(\n",
        "    model = model,\n",
        "    puNG = NG_pu,\n",
        "    jet_ng= NG_jet,\n",
        "    image_size = bins,  # Size of your images (ensure your images are square)\n",
        "    timesteps = 200,  # Number of diffusion steps\n",
        "    objective = \"pred_x0\",\n",
        "    sampling_timesteps = None\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "J67S9C3aurgz",
        "outputId": "30200af4-c248-4f89-ad85-58a645c2ee00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:02<00:00, 91.95it/s]\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADXklEQVR4Ae3bO5IWNxTF8Q+7Mc8ZU84JqGINXh/LYgssggwoqgZDgh/4BRHnN0FHuomCM9EpSVe6/VeXTl31N3deXL7/ff4uLtdR/0X9GHUnShv1Nb1foo6oH6L+j3oe9TbqQ5SINO0lmuB0P0qwBKcEpvF9B6cEjzMP+D2z/hnFIUDXdjfjzrzn7/T+G0VwjVdpfBZltTTtJZrgdD9KsASnBKbxfQenBI/3mUGt4bz/I70ch1eoKzLswl34h23iUSoWFdBvmcZqYtO5l2iC0/0owRKcEpjG9x2cEjzuZYabKBWGGy+o+UwCbvkHPxLBj8TqVfc8yIRns6RzL+FB9sor2TTBoFgUJbgILmElGBSLYnuCx/08ma8ZabrIn+M45VUnIvSa76d0qzV4CifRJkIGmWQv0QSn+1GCJTglMI3vOzgleDjvVSKUmoQHWJKTuOcyn17KOHdf7sMeZmp+1C0OlEVRgovgElaCQbEoSnARXMK2J+jgv6g6/kn+Tn71glOeu1AmFIuCcdZ4lNVUJ8aJzbC9RBOc7kcJluCUwDS+7+CU4MEX1AZqCP5B8QDjbAT/EGGcWONk4Cv8VZ7LzGnaSzTB6X6UYAlOCUzj+w5OCR7Odt8mnOhOeW5AcYMzJTW92ihrnHlZtxipNVWCa9xElSAWa6oE17iJ2p7g4ZR3o8QrKM/kFowHGEf52uKbCB5uwdQu1jCzCL1bqSY43Y4SLMEpgWl838EpwePsHHfy6/UN42xJ/qFX7FmNo/7w21/1EX/rFiO6pkpwjZuoEsRiTZXgGjdR2xM8nPJyVRF4EhWGasJ5z2dUNvxIhPn0Wvdhuv02WW869xJNcLofJViCUwLT+L6DU4KnNQk3oD5nJf7xV9r4TJpu/beiikXV4b8VPyWEg1mjWxw8i6IEF8ElrASDYlGU4CK4hG1P8LhOrm6j+IJaQ5XwMRGPojynWfgHxSt8bXmcWfT+kjYzp2kv0QSn+1GCJTglMI3vOzgleNxkBuf9h7T5mqGacC+lrkjARRWjrjBOm1+KqWys+zQTdouDYlGU4CK4hJVgUCyKElwEl7DtCR6/JteXUW+irqI4hMqBG1C8R2XzJLP8fKJUO+/S+zpqe4JNMHu1KEpwEVzCSjAoFkUJLoJL2DeJSIGkClUT7gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADXklEQVR4Ae3bO5IWNxTF8Q+7Mc8ZU84JqGINXh/LYgssggwoqgZDgh/4BRHnN0FHuomCM9EpSVe6/VeXTl31N3deXL7/ff4uLtdR/0X9GHUnShv1Nb1foo6oH6L+j3oe9TbqQ5SINO0lmuB0P0qwBKcEpvF9B6cEjzMP+D2z/hnFIUDXdjfjzrzn7/T+G0VwjVdpfBZltTTtJZrgdD9KsASnBKbxfQenBI/3mUGt4bz/I70ch1eoKzLswl34h23iUSoWFdBvmcZqYtO5l2iC0/0owRKcEpjG9x2cEjzuZYabKBWGGy+o+UwCbvkHPxLBj8TqVfc8yIRns6RzL+FB9sor2TTBoFgUJbgILmElGBSLYnuCx/08ma8ZabrIn+M45VUnIvSa76d0qzV4CifRJkIGmWQv0QSn+1GCJTglMI3vOzgleDjvVSKUmoQHWJKTuOcyn17KOHdf7sMeZmp+1C0OlEVRgovgElaCQbEoSnARXMK2J+jgv6g6/kn+Tn71glOeu1AmFIuCcdZ4lNVUJ8aJzbC9RBOc7kcJluCUwDS+7+CU4MEX1AZqCP5B8QDjbAT/EGGcWONk4Cv8VZ7LzGnaSzTB6X6UYAlOCUzj+w5OCR7Odt8mnOhOeW5AcYMzJTW92ihrnHlZtxipNVWCa9xElSAWa6oE17iJ2p7g4ZR3o8QrKM/kFowHGEf52uKbCB5uwdQu1jCzCL1bqSY43Y4SLMEpgWl838EpwePsHHfy6/UN42xJ/qFX7FmNo/7w21/1EX/rFiO6pkpwjZuoEsRiTZXgGjdR2xM8nPJyVRF4EhWGasJ5z2dUNvxIhPn0Wvdhuv02WW869xJNcLofJViCUwLT+L6DU4KnNQk3oD5nJf7xV9r4TJpu/beiikXV4b8VPyWEg1mjWxw8i6IEF8ElrASDYlGU4CK4hG1P8LhOrm6j+IJaQ5XwMRGPojynWfgHxSt8bXmcWfT+kjYzp2kv0QSn+1GCJTglMI3vOzgleNxkBuf9h7T5mqGacC+lrkjARRWjrjBOm1+KqWys+zQTdouDYlGU4CK4hJVgUCyKElwEl7DtCR6/JteXUW+irqI4hMqBG1C8R2XzJLP8fKJUO+/S+zpqe4JNMHu1KEpwEVzCSjAoFkUJLoJL2DeJSIGkClUT7gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADXklEQVR4Ae3bO5IWNxTF8Q+7Mc8ZU84JqGINXh/LYgssggwoqgZDgh/4BRHnN0FHuomCM9EpSVe6/VeXTl31N3deXL7/ff4uLtdR/0X9GHUnShv1Nb1foo6oH6L+j3oe9TbqQ5SINO0lmuB0P0qwBKcEpvF9B6cEjzMP+D2z/hnFIUDXdjfjzrzn7/T+G0VwjVdpfBZltTTtJZrgdD9KsASnBKbxfQenBI/3mUGt4bz/I70ch1eoKzLswl34h23iUSoWFdBvmcZqYtO5l2iC0/0owRKcEpjG9x2cEjzuZYabKBWGGy+o+UwCbvkHPxLBj8TqVfc8yIRns6RzL+FB9sor2TTBoFgUJbgILmElGBSLYnuCx/08ma8ZabrIn+M45VUnIvSa76d0qzV4CifRJkIGmWQv0QSn+1GCJTglMI3vOzgleDjvVSKUmoQHWJKTuOcyn17KOHdf7sMeZmp+1C0OlEVRgovgElaCQbEoSnARXMK2J+jgv6g6/kn+Tn71glOeu1AmFIuCcdZ4lNVUJ8aJzbC9RBOc7kcJluCUwDS+7+CU4MEX1AZqCP5B8QDjbAT/EGGcWONk4Cv8VZ7LzGnaSzTB6X6UYAlOCUzj+w5OCR7Odt8mnOhOeW5AcYMzJTW92ihrnHlZtxipNVWCa9xElSAWa6oE17iJ2p7g4ZR3o8QrKM/kFowHGEf52uKbCB5uwdQu1jCzCL1bqSY43Y4SLMEpgWl838EpwePsHHfy6/UN42xJ/qFX7FmNo/7w21/1EX/rFiO6pkpwjZuoEsRiTZXgGjdR2xM8nPJyVRF4EhWGasJ5z2dUNvxIhPn0Wvdhuv02WW869xJNcLofJViCUwLT+L6DU4KnNQk3oD5nJf7xV9r4TJpu/beiikXV4b8VPyWEg1mjWxw8i6IEF8ElrASDYlGU4CK4hG1P8LhOrm6j+IJaQ5XwMRGPojynWfgHxSt8bXmcWfT+kjYzp2kv0QSn+1GCJTglMI3vOzgleNxkBuf9h7T5mqGacC+lrkjARRWjrjBOm1+KqWys+zQTdouDYlGU4CK4hJVgUCyKElwEl7DtCR6/JteXUW+irqI4hMqBG1C8R2XzJLP8fKJUO+/S+zpqe4JNMHu1KEpwEVzCSjAoFkUJLoJL2DeJSIGkClUT7gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADXklEQVR4Ae3bO5IWNxTF8Q+7Mc8ZU84JqGINXh/LYgssggwoqgZDgh/4BRHnN0FHuomCM9EpSVe6/VeXTl31N3deXL7/ff4uLtdR/0X9GHUnShv1Nb1foo6oH6L+j3oe9TbqQ5SINO0lmuB0P0qwBKcEpvF9B6cEjzMP+D2z/hnFIUDXdjfjzrzn7/T+G0VwjVdpfBZltTTtJZrgdD9KsASnBKbxfQenBI/3mUGt4bz/I70ch1eoKzLswl34h23iUSoWFdBvmcZqYtO5l2iC0/0owRKcEpjG9x2cEjzuZYabKBWGGy+o+UwCbvkHPxLBj8TqVfc8yIRns6RzL+FB9sor2TTBoFgUJbgILmElGBSLYnuCx/08ma8ZabrIn+M45VUnIvSa76d0qzV4CifRJkIGmWQv0QSn+1GCJTglMI3vOzgleDjvVSKUmoQHWJKTuOcyn17KOHdf7sMeZmp+1C0OlEVRgovgElaCQbEoSnARXMK2J+jgv6g6/kn+Tn71glOeu1AmFIuCcdZ4lNVUJ8aJzbC9RBOc7kcJluCUwDS+7+CU4MEX1AZqCP5B8QDjbAT/EGGcWONk4Cv8VZ7LzGnaSzTB6X6UYAlOCUzj+w5OCR7Odt8mnOhOeW5AcYMzJTW92ihrnHlZtxipNVWCa9xElSAWa6oE17iJ2p7g4ZR3o8QrKM/kFowHGEf52uKbCB5uwdQu1jCzCL1bqSY43Y4SLMEpgWl838EpwePsHHfy6/UN42xJ/qFX7FmNo/7w21/1EX/rFiO6pkpwjZuoEsRiTZXgGjdR2xM8nPJyVRF4EhWGasJ5z2dUNvxIhPn0Wvdhuv02WW869xJNcLofJViCUwLT+L6DU4KnNQk3oD5nJf7xV9r4TJpu/beiikXV4b8VPyWEg1mjWxw8i6IEF8ElrASDYlGU4CK4hG1P8LhOrm6j+IJaQ5XwMRGPojynWfgHxSt8bXmcWfT+kjYzp2kv0QSn+1GCJTglMI3vOzgleNxkBuf9h7T5mqGacC+lrkjARRWjrjBOm1+KqWys+zQTdouDYlGU4CK4hJVgUCyKElwEl7DtCR6/JteXUW+irqI4hMqBG1C8R2XzJLP8fKJUO+/S+zpqe4JNMHu1KEpwEVzCSjAoFkUJLoJL2DeJSIGkClUT7gAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate samples\n",
        "batch_size = 4\n",
        "NG_jet.reset()\n",
        "NG_pu.reset()\n",
        "sampled_images = diffusion.sample(batch_size=batch_size)\n",
        "show_tensor_images(sampled_images, scale_factor=10)\n",
        "# pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# show_tensor_images(pats, scale_factor=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "nOAtIgKw1rFE",
        "outputId": "b1d94352-bd06-4dbd-d7c6-08cc38538455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fresh training\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:46<00:00, 15.28it/s, Loss=0.0256]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_1_loss_0.0256.pth\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:46<00:00, 15.13it/s, Loss=0.0033]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_2_loss_0.0033.pth\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.07it/s, Loss=0.0032]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_3_loss_0.0032.pth\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.06it/s, Loss=0.0036]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_4_loss_0.0036.pth\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.04it/s, Loss=0.0004]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_5_loss_0.0004.pth\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.03it/s, Loss=0.0009]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_6_loss_0.0009.pth\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.02it/s, Loss=0.0053]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_7_loss_0.0053.pth\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.02it/s, Loss=0.0002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_8_loss_0.0002.pth\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.03it/s, Loss=0.0030]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_9_loss_0.0030.pth\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.01it/s, Loss=0.0001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_10_loss_0.0001.pth\n",
            "Finished training\n"
          ]
        }
      ],
      "source": [
        "# this one is to be passed into DataLoader for training\n",
        "train_batch_size = 100\n",
        "ng_for_dataloader = NGenForDataloader(NG_jet)\n",
        "# validation_jets = NGenForDataloader(NG_jet)\n",
        "# ng_for_dataloader = NGenForDataloader(NG_jet, batch_size=100)\n",
        "# We handle batching ourselves, by serving \"batch_size\" number of jets to train on each time\n",
        "dataloader = DataLoader(ng_for_dataloader, batch_size=train_batch_size, num_workers=2, shuffle = True, pin_memory = True)\n",
        "# validation_loader = DataLoader(ng_for_dataloader, batch_size=100, num_workers=2, shuffle = True, pin_memory = True)\n",
        "# dataloader = DataLoader(ng_for_dataloader, pin_memory = True)\n",
        "# print(dataloader)\n",
        "#print(ng_for_dataloader)\n",
        "# pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# show_tensor_images(pats, scale_factor=10)\n",
        "# print(tt.select_event(0).shape)\n",
        "save_dir = f\"{CWD}/data/ML/second\"\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "# print(f\"{len(ng_for_dataloader)} jets, {pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)ng_for_dataloader.batch_size} batches, {num_epochs} epochs\")\n",
        "load_and_train(diffusion, dataloader, num_epochs=num_epochs, device=device, save_dir=save_dir, lr=1e-4)\n",
        "# trainer_params = {\n",
        "#         diffusion_model: PUDiffusion,\n",
        "#         dataloader: dataloader,\n",
        "#         train_batch_size: train_batch_size,\n",
        "#         gradient_accumulate_every: 1,\n",
        "#         augment_horizontal_flip : False,\n",
        "#         train_lr: 1e-4,\n",
        "#         train_num_steps : 200,\n",
        "#         ema_update_every : 10,\n",
        "#         ema_decay : 0.995,\n",
        "#         adam_betas: (0.9, 0.99),\n",
        "#         save_and_sample_every : 5,\n",
        "#         num_samples : 25,\n",
        "#         results_folder : './ML/results',\n",
        "#         amp : False,\n",
        "#         mixed_precision_type : 'fp32',\n",
        "#         split_batches : True,\n",
        "#         convert_image_to :None,\n",
        "#         calculate_fid : False,\n",
        "#         inception_block_idx : 2048,\n",
        "#         max_grad_norm : 1.0,\n",
        "#         num_fid_samples : 50000,\n",
        "#         save_best_and_latest_only : False\n",
        "# }\n",
        "# trainer = PUTrainer(**trainer_params)\n",
        "# trainer = PUTrainer(\n",
        "#   diffusion,\n",
        "#   dataloader,\n",
        "#   train_batch_size = train_batch_size,\n",
        "#   gradient_accumulate_every = 1,\n",
        "#   augment_horizontal_flip = True,\n",
        "#   train_lr = 1e-6,\n",
        "#   train_num_steps = 100,\n",
        "#   ema_update_every = 10,\n",
        "#   ema_decay = 0.995,\n",
        "#   adam_betas = (0.9, 0.99),\n",
        "#   save_and_sample_every = 20,\n",
        "#   num_samples = 100,\n",
        "#   results_folder = f'{CWD}/data/ML/results',\n",
        "#   amp = False,\n",
        "#   mixed_precision_type = 'fp32',\n",
        "#   split_batches = True,\n",
        "#   convert_image_to = None,\n",
        "#   calculate_fid = True,\n",
        "#   inception_block_idx = 2048,\n",
        "#   max_grad_norm = 1.,\n",
        "#   num_fid_samples = 50000,\n",
        "#   save_best_and_latest_only = False\n",
        "#   )\n",
        "# trainer.train()\n",
        "print(\"Finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aM0CGsUD1wKQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PUDiffusion(\n",
              "  (model): Unet(\n",
              "    (init_conv): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (time_mlp): Sequential(\n",
              "      (0): SinusoidalPosEmb()\n",
              "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (downs): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): Attention(\n",
              "          (norm): RMSNorm()\n",
              "          (attend): Attend(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (ups): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Attention(\n",
              "          (norm): RMSNorm()\n",
              "          (attend): Attend(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (mid_block1): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Identity()\n",
              "    )\n",
              "    (mid_attn): Attention(\n",
              "      (norm): RMSNorm()\n",
              "      (attend): Attend(\n",
              "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (mid_block2): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Identity()\n",
              "    )\n",
              "    (final_res_block): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "diffusion.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:02<00:00, 99.05it/s]\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiilwaTFFFFFFFFFFFFFFFFFFFPpp60hoooooooooooooooFLS0tNPWkNFFFFFFFFFFFFFFFFLRk0maKKKKKKKKKKKKKKKKKM0UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUV//2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA4UlEQVR4Ae3XQQrEIAwF0E479z/ytDvdZPOJDBZeVyEkQZ+C9Dh8BAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAi8S+CzernXGPgb0T2iPDjzlv92WGDXmyDBrkC33x3sCn47A+YzNKPOvKrXEVcqSY5golXVEqxUkhzBRKuq3V5w0RMwx3T+P14puP0RW2B1r5IcwUSrqiVYqSS57QWTzaglQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAgTUCDxy4Az9XStZlAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate samples\n",
        "NG_jet.reset()\n",
        "NG_pu.reset()\n",
        "sampled_images = diffusion.sample(batch_size=1)\n",
        "show_tensor_images(sampled_images, scale_factor=10)\n",
        "# next(NG_pu)\n",
        "# next(NG_jet)\n",
        "# sampled_images = diffusion.sample(batch_size=1)\n",
        "# show_tensor_images(sampled_images, scale_factor=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70901\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit3/AIRv/p7/APIf/wBesm9tvsl28G/ftx82MZyM1BRRRRRRRRRRRRRRRRRXWf2zp/8Az8f+ON/hXPanNHcahLLE25Gxg4x2FVKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK9P8ACXwf/wCEp8MWes/279l+07/3P2Tft2uy9d4z93PTvWL8QPh//wAIL/Z3/Ez+3fbPN/5d/K2bNv8AtNnO79K4qiiiiiiiiiiiiiiiiivd/h38RPCuheBNN03UtU8i7h83zI/s8rYzK7DlVI6Ed65f4weLdD8U/wBjf2Nffavs3n+b+6dNu7y8feUZ+6enpXmFLg0/yJP7v60xlKnBGDSUUUUUUUUUUUUUUUUo606rtU5/9c1R0UUUUUUUUUUUUUUU6P74qevaa8p8Y/8AI1Xv/AP/AEBaw6KKKKKKKKKKKKKK6jwfpFjqv237bB5vl7NnzsuM7s9CPQVqeIfD2l6foVzdWtr5cybdreYxxlgDwTjoa4XzG9a2v+Ex17/n/wD/ACCn/wATWVeXlxqF291dSeZM+NzYAzgYHA46CoKKKKKKKKKKKKKKK2vD/iD+wvtH+i+f523/AJabcYz7H1q9q/jD+1dLmsvsPleZt+fzt2MMD02j0rl6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK7PQv+QNb/wDAv/QjWd4p/wCXT/gf/stc7RRX0v8ACn/kmukf9tv/AEc9cZ8ef+Zf/wC3n/2lXjdFFFFFFFFFFFFXYNWvbaFYYZ9sa5wNinvnuKiur+5vdn2iTfszt+UDGfp9Kr0UV02lfEDxRoemQ6dp2p+TaQ7tkf2eJsZJY8spPUmqWveK9b8T/Z/7YvftP2fd5X7pE27sZ+6Bn7o6+lY1FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAABL0lEQVR4Ae3ZOwrCUBQEUH/YuRN7C1tXYuliLF2NrYW9C3EJIsKFgM3L+A0n1SQwSd7hQkgyGtkIECBAgAABAgQIECDwPYHxqy+9rhOeKiVhkpQ/0XWDqTJBgqlA2jeDqaA+AQIECBAgQIAAAQIECBAgQIAAgf8V6PGf5FKrXVZ6X/ABM7UlSDAVSPtmcPCCPZ4kz0ymdXBR6VopCWYw0bt3CRJMBdK+GRy84Cxd4aM/r9NsK+0rJcEMJnr3LkGCqUDaN4ODF4zeSXbFc6j06mAGU1GCBFOBtG8GBy+YLlCfAAECBAgQIECAAAECiUCPr1ubut6x0rNwroOrSu3Be3G7WbdBsOvRvkew3azbINj1aN/7ecH2JWkQIECAAAECBAgQIECAAAECBAgQ+BWBGycMB2zzraOaAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigAk4FO2N6Um0+lJ0oooooooooooooooopyffFTVHTG+9SUUUUUUUUUUUUUUUVNa/8fKfj/KtKqNV5P9YabRRRRRRRRRRRRRRRSo7IwZTgipftU39/9BUfmP6/pSEknJ60lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA0ElEQVR4Ae3VuwqAMAwF0CoOjv7/Zzo7OAiRglMiPuB0Sgs3tKeFtmYQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIPAvgeGe7c7RZruoYilRjInMoxEbrHITJFgVqOa9wargVG1w5Jdos0Z1/imxlChccQKtixDsOBITggm0LkKw40hMPi+YOJMIAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKvCuyf4AQodGH7UgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiinBGIyBxSMpXqKSiiiiiiiiiiiiiiiiip4/9WKbN/DUVFFFFFFFFFFFFFFFFFFFFFFFPj7049KbTKKKKKKKKKKKKKKKKKUEjpRuPrRk0lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAAy0lEQVR4Ae3VMQqAMAwAwOrg4P/f6ia4ZcoSY6HCdQohacO10DEsAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgT+JbDVxz2j5YpoXrDP2/qbnQ3YdSRIsCvQ7fcGu4IvfpL6kUe03EkUqTRwxSlLIUmwgJWWEkxZCkmCBay0dHnBdGpJAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQWFngAAiQDJxK7K9IAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiinKjPnaM4oMbqMkcU2iiiiiiiiiiiiiiiiiprf+L8Kkm/1RqrRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRSjk07aPSmUUUUUUUUUUUUUUUUUUA4NO3e1Noooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooor//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAAy0lEQVR4Ae3XMQqAMAwAwCqu/v+hrg7dMiZIKihcp0CSNhwd2jEsAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgT+JbB1xj2j+YpodbCv3nD1fgbsihIk2BXo9ruDXUH9BAgQIECgJ9D6kzw/+oiWO6I88FjIfeoswdooryCY+9RZgrVRXvF5wXx8WQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBB4Q2ACUjsCJ5swXicAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiilpKKKKKKKKKKKKKKKKKKVfvU+o6KKKKKKKKKKKKKKKKKUdadTKKKKKKKKKKKKKKKKUDIoIxQDzS5ptFFFFFFFFFFFFFFFOBGKQnNJS0lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFS7F9KjYYYgUlFFFFFFFFFFFFFFFA5NLt96mqJ/vmm0UUUUUUUUUUUUUUUo60uRT96+tRscsSKSiiiiiiiiiiiiiiiiiiiiiiilpDRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAABMUlEQVR4Ae3Z7QqCQBAF0L6g93/coCgi2EEIYr0brHL6NUp3WI8Dsno4+BEgQIAAAQIECBAgQIAAAQIECBDYqsBxzMKrzXNMw9bl1KpJCwtMbwxBgqlAmjeDqeAlbfDJn1ube6vGFG5x6kiQYCqQ5s3g7gWjJ8n/diLlbgbLYl1FcJ1bpQiWxbqK4Dq3Sk0vWA+DWvSPqvYfj/bPauPtVkOZpJh+Bi0wnRSCBFMBeQIECBAgQIAAAQIECGxZoF5LDbqKa+tza1VS2Lgneu8sQYKpQJo3g7sXjL64l059O6lzYyozmDoSJJgKpHkzuHvB4XuSfrFawrev9WawX3SZILj06D8i2G+2TBBcevQfTS/Yf0kSBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF9CLwAZKoHkSRGLYwAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigUtJRRRRRRRRRRRRRRRRRSig9KSiiiiiiiiiiiiiiiiigHFLmkooooooooooooooooooooooooooooooooooooooooooooooooooooop8P+tFWqpUUUUUUUUUUUUUUUUUUqkhgR1qTzH9f0qKiiiiiiiiiiiiiiiiigcGl3e1JRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA50lEQVR4Ae3Yuw6CQBRFUXy0/v+HWhvthhayR0J0Ud2CM481lxBYFhcBAgQIECBAgAABAgQIECBAgAABAgQIECBA4D8FLkdse53kvXu66+7EwQELrOAECVaBmteDVfBeB9iS9ybZovStezwkVZYgwSpQ86fvwbpBeQIECBAgQIAAAQIECBSB9cdTGWV5jPRzVHOK03/VWWA9aIIEq0DN68EqOOlNchvreI1qTuGIqyNBglWg5vXgzwvWDcoTIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwH6BD5obBFCdYRlkAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiijBooooooooooooooooooop1NPWiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiilwaNjHtQVI60lFFFFFFFFFFFFFFFFPp6/dpknamUUUUUUUUUUUUUUUUUUUUUUVs2H/HlH+P8zVfVf+WX4/0rOoooooooooooooooooqeO7niQIj4UdBgU2aeSbHmNux04AqKiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiv/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA/ElEQVR4Ae3ZTQ6CMBAGUPw5ifc/k/dwo+6GzK4zDYj6WH1N+GjzaNIElsVFgAABAgQIECBAgAABAgQIECBA4FsFTqMLP8eNz0h7hHXePWZrzGGBDbRUIZg4GgOCDbRUIZg4DAgQIECAAAECBAgQIECAAAECBAj8lcDwf5JLsFwjPSJtF3zAnLUlSHBWYLZvD/684PBJUpe4ReUeqR7swbpZbhDMHvURwbpZbhDMHvXR4QU3PEnWR7/qcNE4vKAFxrtqBoJNuKgRDIpmINiEUyNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBDwq8AU/MBE1e9e58AAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigAk4FO2N6Um0+lJ0oooooooooooooooopyffFTVHTG+9SUUUUUUUUUUUUUUUVNa/8fKfj/KtKqNV5P9YabRRRRRRRRRRRRRRRSo7IwZTgipftU39/9BUfmP6/pSEknJ60lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA0ElEQVR4Ae3VuwqAMAwF0CoOjv7/Zzo7OAiRglMiPuB0Sgs3tKeFtmYQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIPAvgeGe7c7RZruoYilRjInMoxEbrHITJFgVqOa9wargVG1w5Jdos0Z1/imxlChccQKtixDsOBITggm0LkKw40hMPi+YOJMIAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKvCuyf4AQodGH7UgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(len(ng_for_dataloader))\n",
        "pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# print(ng_for_dataloader[0])\n",
        "ng_for_dataloader_unscaled = ng_for_dataloader[0] * NG_jet.max_energy\n",
        "show_tensor_images(ng_for_dataloader_unscaled, scale_factor=10)\n",
        "show_tensor_images(pats, scale_factor=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pvdsNEqC_RoD"
      },
      "outputs": [],
      "source": [
        "# class PileupDiffusion(GaussianDiffusion):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         model,\n",
        "#         *,\n",
        "#         image_size,\n",
        "#         timesteps = 1000,\n",
        "#         sampling_timesteps = None,\n",
        "#         objective = 'pred_v',\n",
        "#         beta_schedule = 'sigmoid',\n",
        "#         schedule_fn_kwargs = dict(),\n",
        "#         ddim_sampling_eta = 0.,\n",
        "#         auto_normalize = True,\n",
        "#         offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
        "#         min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n",
        "#         min_snr_gamma = 5,\n",
        "#         immiscible = False\n",
        "#     ):\n",
        "#         super().__init__()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f901c8354c4a168759d830710b5bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12c9b8cba5bb44109c3d460a8bb9eaec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170d582725ff42ee92bf089a5e99db82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f594a87e924046bbb046ac798b4be159",
            "placeholder": "​",
            "style": "IPY_MODEL_c3b46ca3a3984adcb72c77ac9790d081",
            "value": "sampling loop time step: 100%"
          }
        },
        "18a78d0165cc4d31ab6f914477a1e38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ae5e9f79e548518f1103d6b2fcc020",
            "placeholder": "​",
            "style": "IPY_MODEL_69645c7b3d44463db12630c2b28d6311",
            "value": " 1000/1000 [01:18&lt;00:00, 12.90it/s]"
          }
        },
        "1b4c24252e69464b8b55219b09f5719f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22562c7b650b460b90d3986d1cf2dbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bbf4414cd6c4b678c117e991ef3500a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea47496db3c40f38a350ba21365d3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e74f819dbd4b81bf1c7ba0f9c15670",
            "placeholder": "​",
            "style": "IPY_MODEL_22562c7b650b460b90d3986d1cf2dbdd",
            "value": "sampling loop time step: 100%"
          }
        },
        "40f855e865d141ffbecdcc909feb5881": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc3db99c8434e708ab613a8de71da86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c517885cfa4c5db1b350775e481a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d9a21a1d7ac4c1082d537a32ab78669",
              "IPY_MODEL_f81e3ba126724d7f95a5140b89c127a6",
              "IPY_MODEL_64899dafc9b748dd985dd1800f5a18c9"
            ],
            "layout": "IPY_MODEL_677cf3826fe5477c9c13f2458ae18788"
          }
        },
        "58ae5e9f79e548518f1103d6b2fcc020": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5904f0aeba2648f0903cb0738963038a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b79306793a24f4c8bd46c78cb515732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61cf806af9cd4a8aab4e64e6bfa81fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_170d582725ff42ee92bf089a5e99db82",
              "IPY_MODEL_c71eb60fe15f4cb7b9182a4b5a677068",
              "IPY_MODEL_98dc9e829f4a4292aac01ab3990be72b"
            ],
            "layout": "IPY_MODEL_5b79306793a24f4c8bd46c78cb515732"
          }
        },
        "627d07804d5e4d99ba2915a52a006796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64899dafc9b748dd985dd1800f5a18c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc34e955ddb9409db13400a9fe1d4af0",
            "placeholder": "​",
            "style": "IPY_MODEL_fd30649a40ff4132b3365373e551173a",
            "value": " 1000/1000 [01:18&lt;00:00, 12.99it/s]"
          }
        },
        "677cf3826fe5477c9c13f2458ae18788": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69645c7b3d44463db12630c2b28d6311": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7341f14b06d74fc7bf57ff66782d8014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75e385e73db94654a515f4ba28586269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ea47496db3c40f38a350ba21365d3ca",
              "IPY_MODEL_e843a479201049b89afe61c4eeee7e22",
              "IPY_MODEL_d9f998afc18a4eadb5bd69b98ee08c9f"
            ],
            "layout": "IPY_MODEL_b53688b74fc64b42b8569d1cc52bba8e"
          }
        },
        "7bfdb8a6411c46deb96094d5de110b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_902a5564e03748fe93b872cdaa8f6d9d",
              "IPY_MODEL_ffbf32f91d7a4ec39df292c13e62ccd6",
              "IPY_MODEL_18a78d0165cc4d31ab6f914477a1e38e"
            ],
            "layout": "IPY_MODEL_fefd1abd335e4775a1a267f0008151c6"
          }
        },
        "902a5564e03748fe93b872cdaa8f6d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c9b8cba5bb44109c3d460a8bb9eaec",
            "placeholder": "​",
            "style": "IPY_MODEL_627d07804d5e4d99ba2915a52a006796",
            "value": "sampling loop time step: 100%"
          }
        },
        "98dc9e829f4a4292aac01ab3990be72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc43b41d36bf4fd1b3b576e06d7880e4",
            "placeholder": "​",
            "style": "IPY_MODEL_7341f14b06d74fc7bf57ff66782d8014",
            "value": " 250/250 [00:19&lt;00:00, 12.68it/s]"
          }
        },
        "9b5861f6266849eeaad610b5f23a6bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d9a21a1d7ac4c1082d537a32ab78669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc465482e8443c495f2156aa5fc4ceb",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc3db99c8434e708ab613a8de71da86",
            "value": "sampling loop time step: 100%"
          }
        },
        "aa1292a549ef4c2ca499d83d9e569381": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc465482e8443c495f2156aa5fc4ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53688b74fc64b42b8569d1cc52bba8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b46ca3a3984adcb72c77ac9790d081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3e74f819dbd4b81bf1c7ba0f9c15670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c041868d084fd2805330058b588fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71eb60fe15f4cb7b9182a4b5a677068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bbf4414cd6c4b678c117e991ef3500a",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01f901c8354c4a168759d830710b5bac",
            "value": 250
          }
        },
        "cc34e955ddb9409db13400a9fe1d4af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc43b41d36bf4fd1b3b576e06d7880e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f998afc18a4eadb5bd69b98ee08c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa1292a549ef4c2ca499d83d9e569381",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4c24252e69464b8b55219b09f5719f",
            "value": " 250/250 [00:39&lt;00:00,  6.57it/s]"
          }
        },
        "dfcebb41e86c4cfb993522f80b7184ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e542a5c11e9648629cd75ceab7caaf3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e843a479201049b89afe61c4eeee7e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f855e865d141ffbecdcc909feb5881",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e542a5c11e9648629cd75ceab7caaf3d",
            "value": 250
          }
        },
        "f594a87e924046bbb046ac798b4be159": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81e3ba126724d7f95a5140b89c127a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4c041868d084fd2805330058b588fa6",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfcebb41e86c4cfb993522f80b7184ba",
            "value": 1000
          }
        },
        "fd30649a40ff4132b3365373e551173a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fefd1abd335e4775a1a267f0008151c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffbf32f91d7a4ec39df292c13e62ccd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5904f0aeba2648f0903cb0738963038a",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b5861f6266849eeaad610b5f23a6bb8",
            "value": 1000
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
