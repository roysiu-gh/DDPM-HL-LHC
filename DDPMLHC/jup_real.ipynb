{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uI7qB4jtoe7",
        "outputId": "7aed7beb-b51d-4ea5-cf81-e2f3931b4e4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
            "/home/physics/phuftc/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU Device: NVIDIA RTX A4000\n",
            "Number of GPUs: 1\n",
            "Remember to use cuda device from here on\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import Subset, Dataset, DataLoader, IterableDataset, TensorDataset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torch.amp import autocast\n",
        "import math\n",
        "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
        "# req torch, torchvision, einops, tqdm, ema_pytorch, accelerate\n",
        "from IPython.display import display\n",
        "from einops import rearrange, reduce, repeat\n",
        "import glob\n",
        "from ema_pytorch import EMA\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from accelerate import Accelerator\n",
        "from pathlib import Path\n",
        "from random import random\n",
        "from functools import partial\n",
        "from collections import namedtuple\n",
        "\n",
        "import os\n",
        "CWD = os.getcwd()\n",
        "\n",
        "# Device stuff\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Remember to use {device} device from here on\")\n",
        "print(os.chdir(\"../\"))\n",
        "# %cd /home/physics/phuqza/E9/DDPM-HL-LHC/\n",
        "from DDPMLHC.config import *\n",
        "from DDPMLHC.calculate_quantities import *\n",
        "from DDPMLHC.data_loading import *\n",
        "from DDPMLHC.generate_plots.overlaid_1d import create_overlay_plots\n",
        "from DDPMLHC.generate_plots.bmap import save_to_bmap\n",
        "\n",
        "\n",
        "# Some functions from denoising_diffusion_pytorch that are required but couldn't import\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YPzvhe08tied"
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(tensor_images, scale_factor=8):\n",
        "    to_pil = T.ToPILImage()\n",
        "    pil_images = [to_pil(image) for image in tensor_images]\n",
        "\n",
        "    for img in pil_images:\n",
        "        # Upscale the image\n",
        "        upscaled_img = img.resize(\n",
        "            (img.width * scale_factor, img.height * scale_factor), \n",
        "            Image.NEAREST  # or Image.BOX for smoother results\n",
        "        )\n",
        "        display(upscaled_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load last checkpoint and train the model\n",
        "# def load_and_train(\n",
        "#     diffusion,\n",
        "#     dataloader,\n",
        "#     num_epochs,\n",
        "#     device,\n",
        "#     save_dir,\n",
        "#     lr=1e-4\n",
        "# ):\n",
        "#     os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "#     # Get last epoch number\n",
        "#     checkpoint_files = glob.glob(os.path.join(save_dir, 'checkpoint_epoch_*.pth'))\n",
        "#     last_epoch = 0\n",
        "#     if checkpoint_files:\n",
        "#         epoch_numbers = []\n",
        "#         for f in checkpoint_files:\n",
        "#             try:\n",
        "#                 epoch_num = int(f.split('epoch_')[1].split('_loss')[0])\n",
        "#                 epoch_numbers.append(epoch_num)\n",
        "#             except:\n",
        "#                 continue\n",
        "#         last_epoch = max(epoch_numbers) if epoch_numbers else 0\n",
        "\n",
        "#     # Load checkpoint if exists\n",
        "#     if last_epoch > 0:\n",
        "#         checkpoint_pattern = os.path.join(save_dir, f'checkpoint_epoch_{last_epoch}_*.pth')\n",
        "#         checkpoint_file = glob.glob(checkpoint_pattern)[0]\n",
        "#         print(f\"Loading checkpoint: {checkpoint_file}\")\n",
        "#         checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "#         diffusion.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     else:\n",
        "#         print(\"Starting fresh training\")\n",
        "\n",
        "#     optimizer = optim.Adam(diffusion.parameters(), lr=lr, betas=(0.9,0.999))\n",
        "#     # batch = len(dataloader)\n",
        "#     # for epoch in range(last_epoch, last_epoch + num_epochs):\n",
        "#     #     running_loss = 0.\n",
        "#     #     last_loss = 0.\n",
        "#     #     print(f\"\\nEpoch {epoch + 1}/{last_epoch + num_epochs}\")\n",
        "#         # diffusion.train(True)\n",
        "#         # for i,batch_data in enumerate(dataloader):\n",
        "#         #     batch_data = batch_data.to(device)\n",
        "#         #     optimizer.zero_grad()\n",
        "#         #     loss = diffusion()  # Ensure targets are correctly defined and batched\n",
        "#         #     loss.backward()\n",
        "#         #     optimizer.step()\n",
        "#         #     running_loss += loss.item()\n",
        "\n",
        "#         #     print(f'Train Loss: {loss.item():.4f}')\n",
        "#         #     if i % 100 == 0:\n",
        "#         #         last_loss = running_loss / 1 # loss per batch\n",
        "#         #         print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "#         #         tb_x = epoch_index * len(training_loader) + i + 1\n",
        "#         #         tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "#         #         running_loss = 0.\n",
        "\n",
        "#         # checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{last_loss:.4f}.pth')\n",
        "#         # torch.save({\n",
        "#         #     'epoch': epoch,\n",
        "#         #     'model_state_dict': diffusion.state_dict(),\n",
        "#         #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         #     'loss': last_loss,\n",
        "#         # }, checkpoint_path)\n",
        "#         # print(f'Checkpoint saved: {checkpoint_path}')\n",
        "#     progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     loss_array = []\n",
        "#     for i, images in progress_bar:\n",
        "#         images = images.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss = diffusion(images)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         loss_array.append(loss)\n",
        "#         running_loss += loss.item()\n",
        "#         # print(running_loss)\n",
        "#         avg_loss = running_loss / (i + 1)\n",
        "#         # avg_loss = np.mean(loss_array)\n",
        "#         progress_bar.set_postfix({'Loss': f'{avg_loss:.4f}'})\n",
        "\n",
        "#     # Save checkpoint at the end of each epoch\n",
        "#     checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{avg_loss:.4f}.pth')\n",
        "#     torch.save({\n",
        "#         'epoch': epoch,\n",
        "#         'model_state_dict': diffusion.state_dict(),\n",
        "#         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#         'loss': avg_loss,\n",
        "#     }, checkpoint_path)\n",
        "#     print(f'Checkpoint saved: {checkpoint_path}')\n",
        "\n",
        "\n",
        "## Custom reimplementation of Trainer from DDPM\n",
        "## Avoid subclassing because we do not want to pass in literal files\n",
        "\n",
        "class PUTrainer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        dataloader,\n",
        "        train_batch_size = 100,\n",
        "        gradient_accumulate_every = 1,\n",
        "        augment_horizontal_flip = True,\n",
        "        train_lr = 1e-4,\n",
        "        train_num_steps = 200,\n",
        "        ema_update_every = 10,\n",
        "        ema_decay = 0.995,\n",
        "        adam_betas = (0.9, 0.99),\n",
        "        save_and_sample_every = 1000,\n",
        "        num_samples = 25,\n",
        "        results_folder = './ML/results',\n",
        "        amp = False,\n",
        "        mixed_precision_type = 'fp32',\n",
        "        split_batches = True,\n",
        "        convert_image_to = None,\n",
        "        calculate_fid = True,\n",
        "        inception_block_idx = 2048,\n",
        "        max_grad_norm = 1.,\n",
        "        num_fid_samples = 50000,\n",
        "        save_best_and_latest_only = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # accelerator\n",
        "\n",
        "        self.accelerator = Accelerator(\n",
        "            split_batches = split_batches,\n",
        "            mixed_precision = mixed_precision_type if amp else 'no'\n",
        "        )\n",
        "\n",
        "        # model\n",
        "\n",
        "        self.model = diffusion_model\n",
        "        self.channels = diffusion_model.channels\n",
        "        # is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
        "\n",
        "        # default convert_image_to depending on channels\n",
        "\n",
        "        # if not exists(convert_image_to):\n",
        "        #     convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)\n",
        "\n",
        "        # sampling and training hyperparameters\n",
        "\n",
        "        # assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
        "        self.num_samples = num_samples\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
        "\n",
        "        self.train_num_steps = train_num_steps\n",
        "        self.image_size = diffusion_model.image_size\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        # dataset and dataloader\n",
        "\n",
        "        # self.ds = Dataset(folder, self.image_size, augment_horizontal_flip = augment_horizontal_flip, convert_image_to = convert_image_to)\n",
        "        # self.ds = dataset\n",
        "        # # assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n",
        "\n",
        "        # dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n",
        "        dl = dataloader\n",
        "        print(dl)\n",
        "\n",
        "        dl = self.accelerator.prepare(dl)\n",
        "        print(len(dl))\n",
        "        self.dl = self.cycle(dl=dl)\n",
        "\n",
        "        # optimizer\n",
        "\n",
        "        self.opt = optim.Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
        "\n",
        "        # for logging results in a folder periodically\n",
        "\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
        "            # print(\"EMA model\", self.ema.ema_model)\n",
        "            self.ema.to(self.device)\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok = True)\n",
        "\n",
        "        # step counter state\n",
        "\n",
        "        self.step = 0\n",
        "\n",
        "        # prepare model, dataloader, optimizer with accelerator\n",
        "\n",
        "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
        "\n",
        "        # FID-score computation\n",
        "\n",
        "        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n",
        "\n",
        "        # if self.calculate_fid:\n",
        "        #     from denoising_diffusion_pytorch.fid_evaluation import FIDEvaluation\n",
        "\n",
        "        #     if not is_ddim_sampling:\n",
        "        #         self.accelerator.print(\n",
        "        #             \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n",
        "        #             \"Consider using DDIM sampling to save time.\"\n",
        "        #         )\n",
        "\n",
        "        #     self.fid_scorer = FIDEvaluation(\n",
        "        #         batch_size=self.batch_size,\n",
        "        #         dl=self.dl,\n",
        "        #         sampler=self.ema.ema_model,\n",
        "        #         channels=self.channels,\n",
        "        #         accelerator=self.accelerator,\n",
        "        #         stats_dir=results_folder,\n",
        "        #         device=self.device,\n",
        "        #         num_fid_samples=num_fid_samples,\n",
        "        #         inception_block_idx=inception_block_idx\n",
        "        #     )\n",
        "\n",
        "        if save_best_and_latest_only:\n",
        "            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n",
        "            self.best_fid = 1e10 # infinite\n",
        "\n",
        "        self.save_best_and_latest_only = save_best_and_latest_only\n",
        "    def cycle(self, dl):\n",
        "        while True:\n",
        "            for data in dl:\n",
        "                yield data\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.accelerator.device\n",
        "\n",
        "    def save(self, i):\n",
        "        if not self.accelerator.is_local_main_process:\n",
        "            return\n",
        "\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.accelerator.get_state_dict(self.model),\n",
        "            'opt': self.opt.state_dict(),\n",
        "            'ema': self.ema.state_dict(),\n",
        "            'scaler': self.accelerator.scaler.state_dict() if self.accelerator.scaler is not None else None,\n",
        "        }\n",
        "\n",
        "        torch.save(data, str(self.results_folder / f'model-step{i}.pth'))\n",
        "\n",
        "    def load(self, milestone):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        data = torch.load(str(self.results_folder / f'model-{milestone}.pth'), map_location=device, weights_only=True)\n",
        "\n",
        "        model = self.accelerator.unwrap_model(self.model)\n",
        "        model.load_state_dict(data['model'])\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.opt.load_state_dict(data['opt'])\n",
        "        if self.accelerator.is_main_process:\n",
        "            self.ema.load_state_dict(data[\"ema\"])\n",
        "\n",
        "        if 'version' in data:\n",
        "            print(f\"loading from version {data['version']}\")\n",
        "\n",
        "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
        "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
        "\n",
        "    def num_to_groups(self,num, divisor):\n",
        "        groups = num // divisor\n",
        "        remainder = num % divisor\n",
        "        arr = [divisor] * groups\n",
        "        if remainder > 0:\n",
        "            arr.append(remainder)\n",
        "        return arr\n",
        "    def train(self):\n",
        "        accelerator = self.accelerator\n",
        "        device = accelerator.device\n",
        "\n",
        "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
        "\n",
        "            while self.step < self.train_num_steps:\n",
        "                self.model.train()\n",
        "\n",
        "                total_loss = 0.\n",
        "\n",
        "                for _ in range(self.gradient_accumulate_every):\n",
        "                    data = next(self.dl).to(device)\n",
        "\n",
        "                    with self.accelerator.autocast():\n",
        "                        loss = self.model(data)\n",
        "                        loss = loss / self.gradient_accumulate_every\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    self.accelerator.backward(loss)\n",
        "\n",
        "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "\n",
        "                self.opt.step()\n",
        "                self.opt.zero_grad()\n",
        "\n",
        "                accelerator.wait_for_everyone()\n",
        "\n",
        "                self.step += 1\n",
        "                if accelerator.is_main_process:\n",
        "                    self.ema.update()\n",
        "                    divisible_by = (self.step % self.save_and_sample_every) == 0\n",
        "                    # print(\"???\")\n",
        "                    if self.step != 0 and divisible_by:\n",
        "                        self.ema.ema_model.eval()\n",
        "\n",
        "                        with torch.inference_mode():\n",
        "                            milestone = self.step // self.save_and_sample_every\n",
        "                            batches = self.num_to_groups(self.num_samples, self.batch_size)\n",
        "                            print(f\"right before all_images_list, step {self.step}\")\n",
        "                            # print(\"tt\", self.ema.ema_model.jetNG)\n",
        "                            # all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
        "                            all_images_list = self.ema.ema_model.sample(batch_size=4)\n",
        "\n",
        "                        # all_images = torch.cat(all_images_list, dim = 0)\n",
        "\n",
        "                        # utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
        "\n",
        "                        # whether to calculate fid\n",
        "\n",
        "                        # if self.calculate_fid:\n",
        "                        #     fid_score = self.fid_scorer.fid_score()\n",
        "                        #     accelerator.print(f'fid_score: {fid_score}')\n",
        "\n",
        "                        # if self.save_best_and_latest_only:\n",
        "                        #     if self.best_fid > fid_score:\n",
        "                        #         self.best_fid = fid_score\n",
        "                        #         self.save(\"best\")\n",
        "                        #     self.save(\"latest\")\n",
        "                        # else:\n",
        "                        if self.step % 10 == 0:\n",
        "                            self.save(milestone)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        accelerator.print('training complete')\n",
        "\n",
        "\n",
        "################################\n",
        "def load_and_train(\n",
        "    diffusion,\n",
        "    dataloader,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    save_dir,\n",
        "    lr=1e-4\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    loss_array = []\n",
        "    # Get last epoch number\n",
        "    checkpoint_files = glob.glob(os.path.join(save_dir, 'checkpoint_epoch_*.pth'))\n",
        "    last_epoch = 0\n",
        "    if checkpoint_files:\n",
        "        epoch_numbers = []\n",
        "        for f in checkpoint_files:\n",
        "            try:\n",
        "                epoch_num = int(f.split('epoch_')[1].split('_loss')[0])\n",
        "                epoch_numbers.append(epoch_num)\n",
        "            except:\n",
        "                continue\n",
        "        last_epoch = max(epoch_numbers) if epoch_numbers else 0\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if last_epoch > 0:\n",
        "        checkpoint_pattern = os.path.join(save_dir, f'checkpoint_epoch_{last_epoch}_*.pth')\n",
        "        checkpoint_file = glob.glob(checkpoint_pattern)[0]\n",
        "        print(f\"Loading checkpoint: {checkpoint_file}\")\n",
        "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "        diffusion.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"Starting fresh training\")\n",
        "\n",
        "    optimizer = optim.Adam(diffusion.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(last_epoch, last_epoch + num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{last_epoch + num_epochs}\")\n",
        "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, images in progress_bar:\n",
        "            images = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = diffusion(images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            avg_loss = running_loss / (i + 1)\n",
        "            progress_bar.set_postfix({'Loss': f'{avg_loss:.7f}'})\n",
        "        loss_array.append(avg_loss)\n",
        "        # Save checkpoint at the end of each epoch\n",
        "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}_loss_{avg_loss:.7f}.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': diffusion.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f'Checkpoint saved: {checkpoint_path}')\n",
        "    # Return array of final losses for plotting\n",
        "    return loss_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 :: Loading original data\n",
            "FINISHED loading data\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "MAX_DATA_ROWS = None\n",
        "\n",
        "# === Read in data\n",
        "print(\"0 :: Loading original data\")\n",
        "tt = np.genfromtxt(\n",
        "    TT_PATH, delimiter=\",\", encoding=\"utf-8\", skip_header=1, max_rows=MAX_DATA_ROWS\n",
        ")\n",
        "pu = np.genfromtxt(\n",
        "    PILEUP_PATH, delimiter=\",\", encoding=\"utf-8\", skip_header=1, max_rows=MAX_DATA_ROWS\n",
        ")\n",
        "tt = EventSelector(tt)\n",
        "pu = EventSelector(pu)\n",
        "print(\"FINISHED loading data\\n\")\n",
        "bins=16\n",
        "# Ground truth ttbar jets\n",
        "NG_jet = NoisyGenerator(TTselector=tt, PUselector=pu, bins=bins, mu=0)\n",
        "# Second one to randomly generate and return pile-up events ONLY\n",
        "## Will use np.random.randint to generate NoisyGenerator.mu and then call next\n",
        "NG_pu = NoisyGenerator(TTselector=tt, PUselector=pu, bins=bins, mu=0, pu_only=True)\n",
        "\n",
        "# class OutData():\n",
        "#     def __init__(self, vector, axis=(0,0)):\n",
        "#         pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NGenForDataloader(Dataset):\n",
        "    def __init__(self, noisy_generator, njets=100):\n",
        "        self.ng = noisy_generator\n",
        "        self.jets = []\n",
        "        self.njets = njets\n",
        "        # next(self.ng)\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.ng._max_TT_no - 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        self.ng.select_jet(idx)\n",
        "        x = torch.from_numpy( self.ng.get_grid() )\n",
        "        # x = x.unsqueeze(0)\n",
        "        x = x.unsqueeze(0)\n",
        "        # self.jets.append(x)\n",
        "        # print(\"x jet img\", x.shape)\n",
        "        # y = x\n",
        "        return x\n",
        "\n",
        "# class NGenForDataloader(IterableDataset):\n",
        "#     def __init__(self, noisy_generator, batch_size=1, nsteps=1000):\n",
        "#         super().__init__()\n",
        "#         self.ng = noisy_generator # NoisyGenerator instance that feeds in jet data\n",
        "#         self.batch_size = batch_size # batch_size should select batch_size number of jets\n",
        "#         self.ng.reset()\n",
        "#         self.nsteps = nsteps\n",
        "#         self.current_jetno = self.ng._next_jetID\n",
        "#         # next(self.ng)\n",
        "    \n",
        "#     # USING THIS TO MAKE IterableDataset for model\n",
        "#     # So dataset is \"streamed in\" continously instead of specifically \"selecting\" a jet\n",
        "#     # Hence in each timestep, \n",
        "#     def generate_jet(self):\n",
        "         \n",
        "#         #     if :\n",
        "#         #         raise RuntimeError(\"Requested jet not in loaded set. Did nothing.\")\n",
        "#         #     # Define empty image tensor\n",
        "#         # while True:\n",
        "#         image_data = []\n",
        "#         # images = torch.empty(size=(self.batch_size,1,self.ng.grid_side_bins,self.ng.grid_side_bins))\n",
        "#         # Ensure that if the remainder of data cannot fit in batch_size, just fit thhe rest of the data into one batch\n",
        "#         jet_range = range(self.ng.\n",
        "# t_jetID, self.ng._next_jetID + batch_size) if self.ng._max_TT_no - self.ng._next_jetID > batch_size else range(self.ng._next_jetID, self.ng._max_TT_no)\n",
        "#         for jet_no in jet_range:\n",
        "#             self.ng.select_jet(jet_no) # Select jet, now self.ng.current_event is set\n",
        "#             image = torch.from_numpy(self.ng.get_grid())# Converted to bins x bins image and now tensor\n",
        "#             image_data.append(image)\n",
        "#         # Now image_data is (self.batch_size, bins, bins)\n",
        "#         image_data = torch.stack(image_data)\n",
        "#         image_data = image_data.unsqueeze(1) # Add channels, set to 1\n",
        "#         # image_data = image_data.to(device)\n",
        "#         self.current_jetno = self.ng._next_jetID\n",
        "#         # print(\"image shape 0:::\", image_data[0,:,:,:])\n",
        "#         return image_data\n",
        "            \n",
        "#     def __iter__(self):\n",
        "#         while self.current_jetno < self.ng._max_TT_no:\n",
        "#         # if \n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#             yield self.generate_jet()\n",
        "#             if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#                 self.ng.reset()\n",
        "#                 self.current_jetno = 0\n",
        "#         # return self\n",
        "#                 print(\"reset\")\n",
        "        \n",
        "        \n",
        "#     def __next__(self):\n",
        "#         # if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#         #     raise StopIteration  # Stops for loop after\n",
        "#         # else: \n",
        "#         while self.current_jetno < self.ng._max_TT_no:\n",
        "#         # if \n",
        "#         #     self.ng.reset() # Reset for next cycle through the dataset\n",
        "#             yield self.generate_jet()\n",
        "#             if self.ng._next_jetID >= self.ng._max_TT_no:\n",
        "#                 self.ng.reset()\n",
        "#                 self.current_jetno = 0\n",
        "#         # raise StopIteration  # Stops for loop after\n",
        "#             # return self.generate_jet()\n",
        "#     def __len__(self):\n",
        "#         return self.nsteps\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         self.ng.select_jet(idx)\n",
        "#         x = torch.from_numpy( self.ng.get_grid() )\n",
        "#         # x = x.unsqueeze(0)\n",
        "#         x = x.unsqueeze(0)\n",
        "#         # y = x\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "51c517885cfa4c5db1b350775e481a4c",
            "9d9a21a1d7ac4c1082d537a32ab78669",
            "f81e3ba126724d7f95a5140b89c127a6",
            "64899dafc9b748dd985dd1800f5a18c9",
            "677cf3826fe5477c9c13f2458ae18788",
            "abc465482e8443c495f2156aa5fc4ceb",
            "4fc3db99c8434e708ab613a8de71da86",
            "c4c041868d084fd2805330058b588fa6",
            "dfcebb41e86c4cfb993522f80b7184ba",
            "cc34e955ddb9409db13400a9fe1d4af0",
            "fd30649a40ff4132b3365373e551173a"
          ]
        },
        "id": "8-L7djUbJRvp",
        "outputId": "3f6a255f-66c5-4f50-8f62-27ac575d10d3"
      },
      "outputs": [],
      "source": [
        "model = Unet(\n",
        "    dim=64,                  # Base dimensionality of feature maps\n",
        "    dim_mults=(1, 2, 4, 8),  # Multipliers for feature dimensions at each level\n",
        "    channels=1,              # E.g. 3 for RGB\n",
        ").to(device)\n",
        "\n",
        "class PUDiffusion(GaussianDiffusion):\n",
        "    def __init__(self, model, image_size, puNG: NoisyGenerator, jet_ng: NoisyGenerator,**kwargs):\n",
        "        super(PUDiffusion, self).__init__(model=model, image_size=image_size, **kwargs)\n",
        "        self.puNG = puNG\n",
        "        self.jetNG = jet_ng\n",
        "        self.channels = model.channels\n",
        "        self.mu_counter = 1\n",
        "    def cond_noise(self, x_shape, noise):\n",
        "        return self.pu_to_tensor(x_shape) if noise is None else noise\n",
        "        # return torch.zeros_like(x_start) if noise is None else noise\n",
        "    def generate_data(self, shape, NG: NoisyGenerator):\n",
        "        \"\"\"\n",
        "        This function generates image data matched to the correct shape\n",
        "        \"\"\"\n",
        "        # Start next jet\n",
        "        next(NG)\n",
        "        selected = NG.get_grid()\n",
        "        # If empty pile-up, return array of 0s instead since model should account for this\n",
        "        if selected.size == 0:\n",
        "            return  \"Error in PUDiffusion.generate_jet\"\n",
        "        # print(selected_pu.shape)\n",
        "        pu_tensor = torch.from_numpy(selected)\n",
        "\n",
        "        pu_tensor = torch.unsqueeze(pu_tensor,0)\n",
        "        # This tensor has dimensions BxCxHxW to match x_start\n",
        "        pu_tensor = torch.unsqueeze(pu_tensor,0)\n",
        "        pu_tensor = pu_tensor.expand(shape[0], shape[1], -1, -1) \n",
        "        # pu_tensor = torch.zeros(shape)\n",
        "        pu_tensor = pu_tensor.to(self.device)\n",
        "        return pu_tensor\n",
        "    def pu_to_tensor(self, shape):\n",
        "        # Select random number of pile-ups (mu) to generate, max 200 for now since HL-LHC expected to do up to this\n",
        "        # We are doing it per batch\n",
        "        # if isinstance(t, int):\n",
        "        #     mu = np.random.randint(low=1, high=200, size=None)\n",
        "        # else:\n",
        "        #     mu = np.random.randint(low=1, high=200, size=None)\n",
        "        mu = 1\n",
        "        # print(mu)\n",
        "        # Align jetIDs for correct centering of pile-up\n",
        "        self.puNG._next_jetID = self.jetNG._next_jetID\n",
        "        NG = self.puNG\n",
        "        NG.mu = mu\n",
        "        # NG.reset()\n",
        "        # next(self.puNG)\n",
        "        pu_tensor = self.generate_data(shape=shape, NG=NG)\n",
        "        return pu_tensor\n",
        "    def jet_to_tensor(self, shape):\n",
        "        NG = self.jetNG\n",
        "        # Align jetIDs for correct centering of pile-up\n",
        "        self.puNG._next_jetID = self.jetNG._next_jetID\n",
        "        # next(NG)\n",
        "        pu_tensor = self.generate_data(shape=shape, NG=self.jetNG)\n",
        "        return pu_tensor\n",
        "    # TODO: ddim_sample???\n",
        "    @torch.inference_mode()\n",
        "    def p_sample(self, x, t: int, x_self_cond = None):\n",
        "        b, *_, device = *x.shape, self.device\n",
        "        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n",
        "        # print(\"batched times\", t)\n",
        "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
        "        ######## MODIFY\n",
        "        noise = self.pu_to_tensor(x.shape) if t > 0 else 0 # no noise if t == 0\n",
        "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
        "        return pred_img, x_start\n",
        "    @autocast('cuda', enabled = False)\n",
        "    def q_sample(self, x_start, t, noise = None):\n",
        "        noise = self.cond_noise(x_shape=x_start.shape, noise=noise)\n",
        "\n",
        "        if self.immiscible:\n",
        "            assign = self.noise_assignment(x_start, noise)\n",
        "            noise = noise[assign]\n",
        "        # print(\"q_sample t\", t)\n",
        "        return (\n",
        "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
        "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
        "        )\n",
        "    @torch.inference_mode()\n",
        "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
        "        # print(\"p sample loop\")\n",
        "        # print(shape)\n",
        "        batch, device = shape[0], self.device\n",
        "\n",
        "        # img = torch.randn(shape, device = device)\n",
        "        # This function is called in PUTrainer to sample jets\n",
        "        # img = self.\n",
        "        # img = torch.zeros(shape)\n",
        "        img = self.jet_to_tensor(shape=shape) # Geenerates a jet\n",
        "        # img = img.to(self.device)\n",
        "        imgs = [img]\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
        "            self_cond = x_start if self.self_condition else None\n",
        "            img, x_start = self.p_sample(img, t, self_cond)\n",
        "            self.mu_counter +=1\n",
        "            imgs.append(img)\n",
        "\n",
        "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "        ret = self.unnormalize(ret)\n",
        "        return ret\n",
        "    @torch.inference_mode()\n",
        "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
        "        (h, w), channels = self.image_size, self.channels\n",
        "        # sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
        "        sample_fn = self.p_sample_loop\n",
        "        return sample_fn((batch_size, channels, h, w), return_all_timesteps = return_all_timesteps)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
        "        # print(\"is ddim\")\n",
        "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
        "\n",
        "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
        "        times = list(reversed(times.int().tolist()))\n",
        "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
        "\n",
        "        img = torch.randn(shape, device = device)\n",
        "        # img = self.cond_noise(x_start, sampling_timesteps)\n",
        "        imgs = [img]\n",
        "\n",
        "        x_start = None\n",
        "\n",
        "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
        "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
        "            self_cond = x_start if self.self_condition else None\n",
        "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n",
        "\n",
        "            if time_next < 0:\n",
        "                img = x_start\n",
        "                imgs.append(img)\n",
        "                continue\n",
        "\n",
        "            alpha = self.alphas_cumprod[time]\n",
        "            alpha_next = self.alphas_cumprod[time_next]\n",
        "\n",
        "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
        "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
        "\n",
        "            # noise = torch.randn_like(img)\n",
        "            # noise = self.cond_noise(self_cond, noi)\n",
        "            noise = self.pu_to_tensor(x_start.shape)\n",
        "\n",
        "            img = x_start * alpha_next.sqrt() + \\\n",
        "                  c * pred_noise + \\\n",
        "                  sigma * noise\n",
        "\n",
        "            imgs.append(img)\n",
        "\n",
        "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
        "\n",
        "        ret = self.unnormalize(ret)\n",
        "        return ret\n",
        "\n",
        "\n",
        "    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):\n",
        "        b, c, h, w = x_start.shape\n",
        "        # print(x_start.shape)\n",
        "        # Select one pile-up at a time for each timestep $t$\n",
        "        # single_pileup = self.pu.select_event(np.random.randint(low=0, high=self.pu.max_ID, size=1))\n",
        "        # print(\"p_losses t\", t)\n",
        "\n",
        "        # noise = self.cond_noise(x_start.shape, noise=noise)\n",
        "        noise = torch.zeros_like(x_start)\n",
        "\n",
        "        # if noise is None: \n",
        "        # noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        # offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n",
        "\n",
        "        # if offset_noise_strength > 0:\n",
        "        #     offset_noise = torch.randn(x_start.shape[:2], device = self.device)\n",
        "        #     noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
        "\n",
        "        # noise sample\n",
        "\n",
        "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
        "\n",
        "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
        "        # and condition with unet with that\n",
        "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
        "\n",
        "        x_self_cond = None\n",
        "        if self.self_condition and random() < 0.5:\n",
        "            with torch.no_grad():\n",
        "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
        "                x_self_cond.detach_()\n",
        "\n",
        "        # predict and take gradient step\n",
        "\n",
        "        model_out = self.model(x, t, x_self_cond)\n",
        "\n",
        "        if self.objective == 'pred_noise':\n",
        "            target = noise\n",
        "        elif self.objective == 'pred_x0':\n",
        "            target = x_start\n",
        "        elif self.objective == 'pred_v':\n",
        "            v = self.predict_v(x_start, t, noise)\n",
        "            target = v\n",
        "        else:\n",
        "            raise ValueError(f'unknown objective {self.objective}')\n",
        "\n",
        "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
        "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
        "\n",
        "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
        "        return loss.mean()\n",
        "    def forward(self, img, *args, **kwargs):\n",
        "        # img = img.squeeze(0)\n",
        "        # print(\"???\", *img.shape)\n",
        "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
        "        assert h == img_size[0] and w == img_size[1], f'height and width of image must be {img_size}'\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        return self.p_losses(img, t, *args, **kwargs)\n",
        "\n",
        "\n",
        "# diffusion = GaussianDiffusion(\n",
        "#     model = model,\n",
        "#     image_size = 16,  # Size of your images (ensure your images are square)\n",
        "#     timesteps = 1000,  # Number of diffusion steps\n",
        "#     objective = \"pred_x0\",\n",
        "# ).to(device)\n",
        "\n",
        "diffusion = PUDiffusion(\n",
        "    model = model,\n",
        "    puNG = NG_pu,\n",
        "    jet_ng= NG_jet,\n",
        "    image_size = bins,  # Size of your images (ensure your images are square)\n",
        "    timesteps = 200,  # Number of diffusion steps\n",
        "    objective = \"pred_x0\",\n",
        "    sampling_timesteps = None\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "J67S9C3aurgz",
        "outputId": "30200af4-c248-4f89-ad85-58a645c2ee00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:02<00:00, 89.70it/s]\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADdklEQVR4Ae2bu24UQRBF19CAMcY2r4CHZAmRESH5g/kjCPAvLBgEXj8wmHfkcwg2qko6uI6uuuf2lE+PulQ1sxuvF1d/767E4itqG/UQ9Q11gbpE/UbdQA2U4idyC7VEPUZdQ00qEmB3Y0IwBLsEuv48g12C44AV7qLOUMeoJ6gNlEKH2cXr3KZbWH6g/qJ2UB9QehmaSyTA7n6EYAh2CXT9eQa7BMczVjhFed5bLzxgVuHJb/6w1vA6qxgrFme97y8Gn6OyxaAoihAsgsMWgqAoihAsgsM2PcHxhlg/ojzRDxmz33STMZX/p5nEVaxOlnjteF1nzAj2GXNlhuYSCbC7HyEYgl0CXX+ewS7BsccKt1HvUS9QT1H2pTYZ+4NS2LVym6x2dDjryq7irGNTqQTY3Y4QDMEuga4/z2CX4LA2+MRanvLOrpi1EjlnzKzhrNWJ796tTo7weg97X1+YzRaDoihCsAgOWwiCoihCsAgO2/QEh2e23ahj4vcrrh3GfOthhjD3OGv9IQXrnl3Wc/YzYwpnHZtKJcDudoRgCHYJdP15BrsEh1WCdYVqi/XtZFl1mBfMQhgWvv9wm/w+7IQLzT3GcsGsXobmEgmwux8hGIJdAl1/nsEuwWFdobKasCaxB2WeueTuZhKzgXnGPpez9/AqXHmXwWwxKIoiBIvgsIUgKIoiBIvgsE1PcFgRmA2+E7+ZxFPeWsPz3jxjn8v1fP/hKt7DCJx1lekJJkCelqIIwSI4bCEIiqIIwSI4bEOEZgh7UJ7tXucbE2sXZ3U4a54xazj7iGCOUNZHrszkXCIBdvcjBEOwS6DrzzPYJTisDVQrVrWa8O2Iv/rwvDdX7OG1JjFHmUnsc53hMAJns8XgKYoQLILDFoKgKIoQLILDNj3B/74C3ifqE5S/8DCn3GfWvGAlotf/fROHGcJ3LHayzDhWLK7CInOJBNjdjxAMwS6Brj/PYJfgWLHCIeoU5bdbZpJ1+eMcxzZKodc6xdxjt8y6x+ySLZZjTYVgjZuuEJRFTYVgjZuu6QmOJcHaq7Lf5IluNvDkd2zddY6ZNaxOvJvfeN0hFrtl0xNMgOxaUYRgERy2EARFUYRgERy28Qr5FuX7CmsNe1/2pcwQKr3rOlR2t8wz/nbkJRGYy7LFQCmKECyCwxaCoCiKECyCwzY9wX/1oIKNF2uYkAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADdklEQVR4Ae2bu24UQRBF19CAMcY2r4CHZAmRESH5g/kjCPAvLBgEXj8wmHfkcwg2qko6uI6uuuf2lE+PulQ1sxuvF1d/767E4itqG/UQ9Q11gbpE/UbdQA2U4idyC7VEPUZdQ00qEmB3Y0IwBLsEuv48g12C44AV7qLOUMeoJ6gNlEKH2cXr3KZbWH6g/qJ2UB9QehmaSyTA7n6EYAh2CXT9eQa7BMczVjhFed5bLzxgVuHJb/6w1vA6qxgrFme97y8Gn6OyxaAoihAsgsMWgqAoihAsgsM2PcHxhlg/ojzRDxmz33STMZX/p5nEVaxOlnjteF1nzAj2GXNlhuYSCbC7HyEYgl0CXX+ewS7BsccKt1HvUS9QT1H2pTYZ+4NS2LVym6x2dDjryq7irGNTqQTY3Y4QDMEuga4/z2CX4LA2+MRanvLOrpi1EjlnzKzhrNWJ796tTo7weg97X1+YzRaDoihCsAgOWwiCoihCsAgO2/QEh2e23ahj4vcrrh3GfOthhjD3OGv9IQXrnl3Wc/YzYwpnHZtKJcDudoRgCHYJdP15BrsEh1WCdYVqi/XtZFl1mBfMQhgWvv9wm/w+7IQLzT3GcsGsXobmEgmwux8hGIJdAl1/nsEuwWFdobKasCaxB2WeueTuZhKzgXnGPpez9/AqXHmXwWwxKIoiBIvgsIUgKIoiBIvgsE1PcFgRmA2+E7+ZxFPeWsPz3jxjn8v1fP/hKt7DCJx1lekJJkCelqIIwSI4bCEIiqIIwSI4bEOEZgh7UJ7tXucbE2sXZ3U4a54xazj7iGCOUNZHrszkXCIBdvcjBEOwS6DrzzPYJTisDVQrVrWa8O2Iv/rwvDdX7OG1JjFHmUnsc53hMAJns8XgKYoQLILDFoKgKIoQLILDNj3B/74C3ifqE5S/8DCn3GfWvGAlotf/fROHGcJ3LHayzDhWLK7CInOJBNjdjxAMwS6Brj/PYJfgWLHCIeoU5bdbZpJ1+eMcxzZKodc6xdxjt8y6x+ySLZZjTYVgjZuuEJRFTYVgjZuu6QmOJcHaq7Lf5IluNvDkd2zddY6ZNaxOvJvfeN0hFrtl0xNMgOxaUYRgERy2EARFUYRgERy28Qr5FuX7CmsNe1/2pcwQKr3rOlR2t8wz/nbkJRGYy7LFQCmKECyCwxaCoCiKECyCwzY9wX/1oIKNF2uYkAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADdklEQVR4Ae2bu24UQRBF19CAMcY2r4CHZAmRESH5g/kjCPAvLBgEXj8wmHfkcwg2qko6uI6uuuf2lE+PulQ1sxuvF1d/767E4itqG/UQ9Q11gbpE/UbdQA2U4idyC7VEPUZdQ00qEmB3Y0IwBLsEuv48g12C44AV7qLOUMeoJ6gNlEKH2cXr3KZbWH6g/qJ2UB9QehmaSyTA7n6EYAh2CXT9eQa7BMczVjhFed5bLzxgVuHJb/6w1vA6qxgrFme97y8Gn6OyxaAoihAsgsMWgqAoihAsgsM2PcHxhlg/ojzRDxmz33STMZX/p5nEVaxOlnjteF1nzAj2GXNlhuYSCbC7HyEYgl0CXX+ewS7BsccKt1HvUS9QT1H2pTYZ+4NS2LVym6x2dDjryq7irGNTqQTY3Y4QDMEuga4/z2CX4LA2+MRanvLOrpi1EjlnzKzhrNWJ796tTo7weg97X1+YzRaDoihCsAgOWwiCoihCsAgO2/QEh2e23ahj4vcrrh3GfOthhjD3OGv9IQXrnl3Wc/YzYwpnHZtKJcDudoRgCHYJdP15BrsEh1WCdYVqi/XtZFl1mBfMQhgWvv9wm/w+7IQLzT3GcsGsXobmEgmwux8hGIJdAl1/nsEuwWFdobKasCaxB2WeueTuZhKzgXnGPpez9/AqXHmXwWwxKIoiBIvgsIUgKIoiBIvgsE1PcFgRmA2+E7+ZxFPeWsPz3jxjn8v1fP/hKt7DCJx1lekJJkCelqIIwSI4bCEIiqIIwSI4bEOEZgh7UJ7tXucbE2sXZ3U4a54xazj7iGCOUNZHrszkXCIBdvcjBEOwS6DrzzPYJTisDVQrVrWa8O2Iv/rwvDdX7OG1JjFHmUnsc53hMAJns8XgKYoQLILDFoKgKIoQLILDNj3B/74C3ifqE5S/8DCn3GfWvGAlotf/fROHGcJ3LHayzDhWLK7CInOJBNjdjxAMwS6Brj/PYJfgWLHCIeoU5bdbZpJ1+eMcxzZKodc6xdxjt8y6x+ySLZZjTYVgjZuuEJRFTYVgjZuu6QmOJcHaq7Lf5IluNvDkd2zddY6ZNaxOvJvfeN0hFrtl0xNMgOxaUYRgERy2EARFUYRgERy28Qr5FuX7CmsNe1/2pcwQKr3rOlR2t8wz/nbkJRGYy7LFQCmKECyCwxaCoCiKECyCwzY9wX/1oIKNF2uYkAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOgrntT/AOQhL+H8hWdP/DTYv9YKs1qVBJ981BN/DUD/AHDUVMqRfuinr3p1FRVPH/qxVXUP+Wf4/wBKgtP+PlPx/lWnXp39k2P/ADw/8fb/ABrB1HR7A38v7j0/jb0HvWfcaPYfL+49f42/xqtJpdlHGWWHDDodx/xqD7JB/c/U1sfYbf8A55/+PGkOnWpOTFz/ALx/xpraXZt1h/8AHj/jUcuk2QjJEH/j7f41X/suz/54/wDjx/xqt/Z1r/zy/wDHj/jThYWoH+q/8eP+NOWwtv8Ann/48acthbFgDH/48ak/s60/55f+PH/Gof7Otf8Anl/48f8AGnCyt1GBHx/vGq91YWzbMx56/wARqnLZwQRmSOPa46HJNVvMb1r3D+zv+mv/AI7/APXrB1HT8X8v730/h9h71m3Njjb+89f4aqTWWYmHmf8AjtVf7P8A+mv/AI7/APXrW+x/9NP0o+x/9NP0qSPT/Mz+9xj/AGf/AK9LLpeYz++/8d/+vVb+yf8Apv8A+Of/AF6g/sb/AKb/APjn/wBej+x/+m//AI5/9emtpWzH77Of9n/69INP2nPm/wDjv/16d9k/2/0pn2D/AKa/+O//AF6he22OV35x7VWuYMbfm9e1VJbXzYym/Ge+Krf2V/02/wDHf/r17H/atl/z2/8AHW/wrD1C/tnvpGWTIOP4T6Cs+5uYX27XzjPY1TuLqGKBnd8KMZOD61R/tWy/57f+On/CtX+0rT/nr/46f8KUahakZEv/AI6f8Kmg1G0XdmX0/hP+FSPqVoyECXn/AHT/AIVF9ut/+en/AI6ai+3W/wDz0/8AHTSfb7Yf8tf/AB00yS9t2xiT/wAdNQyX1uiFmkwB/smof7Vsv+e3/jrf4Uv9p2f/AD2/8dP+FV5L62aQkSZH+6aqXd7bjZmT1/hNQJdwSOER8segwamyK66s+5/4+G/D+VV27VS1L/kHy/h/MVz9bNSp90VJH3qQdaWoqY33qBUN3/x6v+H86y6kpw6VVvf+Wf4/0pll/wAfcf4/yNa9egVE/wB81Rv/APln+P8ASs6f/Ut+H86pVt1G/wB406LvUh6U2ikpRUV3/wAer/h/Osum04dKz9T/AOWX4/0qnD/rlq3Xoe0elULmR1uGVTgDH8qpXTs2zJz1qnLzEQarbR6Vc8+T+9+lODswyTzTg7L0NKZX/vfpSea/r+lSbj60xnYHrSo7HPNMuWJt2BPp/OqFZf2qb+/+gpy3M2Pv/oKr3crvs3NnGe1V1dlOQeaf9ol/vfoK9I+1/wCx+tUriXfOzYxnHf2qheTeXs+XOc96py3X7s/J+tV/tX+x+tWvO/2f1p63GBjZ+tOW4z/D+tKZuPu/rSef/s/rUn2n/Y/WjzN3OMU9G68Uk3zREdKreV/tfpWR9m/2/wBKcLfj736VUvl8ry+c5z/Sqe/2o3+1ej1Xl/1hrP1D/ln+P9KoSDchA61D5b+n61d2N6U4ROR939afHDIc/L+tPMEmPu/qKb5Mn939aXyX/u/rUqRuFHH609EYZ4pzRuwwBzTPs0v9z9RWPtPpRkLwetZ+pEHyse/9KoE4GT0pvmL616ZVWb/Wms/UP+Wf4/0qlRVypU+6Kmi709vu0yinDpThT0++KlrnKgk++ao3/wDyz/H+lZ8n+rNQV6xVWb/WmqF//wAs/wAf6VQl/wBWar1qUtOWhvu0ylqeP7gpWoT7wqSqdUp/9c34fyrK1X/lj+P9Kzk+8Klr0LzG9agkYlzzVDUHYeXz6/0rOmkbyjzVXzX9f0q59pm/v/oKcLiXH3/0FTwTO27LZ6dqmDEnBNOpMmpUY7BSsx9aRWOetP3t61S3t61SuJG89ufT+VZmpMW8rJ9f6VSU/MKfk16HioXX5zzVK+i3+XzjGe30qhNb/um+b9Kq/Z/9r9Km2e9TR2+5Ad2PwqxBBt3fNn8KmEXPX9Kd5fvSeX7/AKVKkfyDn9Kd5O7+L9KQwbRndn8Kbiqvl+/6VWltN8hbfjPtWXqtv5Xk/NnO7t9KzlT5hzUmz3r0DevrUTyLvPNU7yVBsy3r2qlLNG0ZAbn6VX3D1p9TRzxogVmwR7Gpo7qHn5/0NSC6hz9/9DTvtMP9/wDQ0/cPWpFkQKATT1lTnn9KJJU2H5v0qDzU/vfpUORUTyIGIJ5+lZGtSIfIwf73b6VmI67xzUu4etdxUL/fNUb/AP5Z/j/SqJ6U2pqif75pU709fvU+tClpy0kn3DUFLVWb/WtWRrH/ACx/4F/Ss2P74qevQKvW/wDqF/H+dZuuf8sP+Bf0rHf7pqKmVah/1QpJe1RUlOpw6VJF3qSlqCoJPvmszVf+WX4/0qjD/rRVqut8+T+9+lU59WvYZmjjnwo6DYv+FUb3VLybZ5k2cZx8o9vaqqXc7MAXyPoKl86T+9+lM86T+9+lSpcyhAA/6ChriVurfoKjknkVCQ3P0FQfa5/7/wCgq150n979KPPk/vfoKPtUy9Hx+ApDe3AH+s/QU37dc/8APT/x0U37ZP8A3/0FTRyO6BmOSaqah83l59/6VTRQHBAqbJrd/tD/AKZf+Pf/AFqrSt50hkxjPaqN83leXxnOf6VWiuP3g+X9asef/s/rTPP/ANn9anjfdGDinFsdqjkfMZ4qvmrm72pN/tSFs9qQfMcdKXyvf9Kd9n/2/wBKsxQ4jA3fpVa+hz5fzevb6VVWD5h836U/yf8Aa/Stj+yr3/nj/wCPL/jUMkbwOY5BtdeozmszVGA8rJ9f6VQSVFcEtx9Kl+1Q/wB/9DU3lP6frU8ZCoFbgilZ19aY7AoeahqzvX1oznkUYNKgO4VLUuRUqOoQAmmSwSXWPJXdt68gY/Oo/wCzrpeTFgf7w/xo+yT/ANz9RXY1zeq/8hKX8P5CsHVv+WP/AAL+lZbfdplb9Mb71Iaa33TUdSVKn3BTxSr94VJTqWtDTP8Alr+H9auTf6o1Vr//2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAADdklEQVR4Ae2bu24UQRBF19CAMcY2r4CHZAmRESH5g/kjCPAvLBgEXj8wmHfkcwg2qko6uI6uuuf2lE+PulQ1sxuvF1d/767E4itqG/UQ9Q11gbpE/UbdQA2U4idyC7VEPUZdQ00qEmB3Y0IwBLsEuv48g12C44AV7qLOUMeoJ6gNlEKH2cXr3KZbWH6g/qJ2UB9QehmaSyTA7n6EYAh2CXT9eQa7BMczVjhFed5bLzxgVuHJb/6w1vA6qxgrFme97y8Gn6OyxaAoihAsgsMWgqAoihAsgsM2PcHxhlg/ojzRDxmz33STMZX/p5nEVaxOlnjteF1nzAj2GXNlhuYSCbC7HyEYgl0CXX+ewS7BsccKt1HvUS9QT1H2pTYZ+4NS2LVym6x2dDjryq7irGNTqQTY3Y4QDMEuga4/z2CX4LA2+MRanvLOrpi1EjlnzKzhrNWJ796tTo7weg97X1+YzRaDoihCsAgOWwiCoihCsAgO2/QEh2e23ahj4vcrrh3GfOthhjD3OGv9IQXrnl3Wc/YzYwpnHZtKJcDudoRgCHYJdP15BrsEh1WCdYVqi/XtZFl1mBfMQhgWvv9wm/w+7IQLzT3GcsGsXobmEgmwux8hGIJdAl1/nsEuwWFdobKasCaxB2WeueTuZhKzgXnGPpez9/AqXHmXwWwxKIoiBIvgsIUgKIoiBIvgsE1PcFgRmA2+E7+ZxFPeWsPz3jxjn8v1fP/hKt7DCJx1lekJJkCelqIIwSI4bCEIiqIIwSI4bEOEZgh7UJ7tXucbE2sXZ3U4a54xazj7iGCOUNZHrszkXCIBdvcjBEOwS6DrzzPYJTisDVQrVrWa8O2Iv/rwvDdX7OG1JjFHmUnsc53hMAJns8XgKYoQLILDFoKgKIoQLILDNj3B/74C3ifqE5S/8DCn3GfWvGAlotf/fROHGcJ3LHayzDhWLK7CInOJBNjdjxAMwS6Brj/PYJfgWLHCIeoU5bdbZpJ1+eMcxzZKodc6xdxjt8y6x+ySLZZjTYVgjZuuEJRFTYVgjZuu6QmOJcHaq7Lf5IluNvDkd2zddY6ZNaxOvJvfeN0hFrtl0xNMgOxaUYRgERy2EARFUYRgERy28Qr5FuX7CmsNe1/2pcwQKr3rOlR2t8wz/nbkJRGYy7LFQCmKECyCwxaCoCiKECyCwzY9wX/1oIKNF2uYkAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate samples\n",
        "batch_size = 4\n",
        "NG_jet.reset()\n",
        "NG_pu.reset()\n",
        "sampled_images = diffusion.sample(batch_size=batch_size)\n",
        "show_tensor_images(sampled_images, scale_factor=10)\n",
        "# pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# show_tensor_images(pats, scale_factor=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "nOAtIgKw1rFE",
        "outputId": "b1d94352-bd06-4dbd-d7c6-08cc38538455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fresh training\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:46<00:00, 15.30it/s, Loss=0.0308578]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_1_loss_0.0308578.pth\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.08it/s, Loss=0.0031812]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_2_loss_0.0031812.pth\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 15.01it/s, Loss=0.0062637]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_3_loss_0.0062637.pth\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.97it/s, Loss=0.0014786]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_4_loss_0.0014786.pth\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.96it/s, Loss=0.0017421]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_5_loss_0.0017421.pth\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.97it/s, Loss=0.0010237]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_6_loss_0.0010237.pth\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.96it/s, Loss=0.0003894]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_7_loss_0.0003894.pth\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.95it/s, Loss=0.0007574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_8_loss_0.0007574.pth\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.92it/s, Loss=0.0009459]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_9_loss_0.0009459.pth\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.95it/s, Loss=0.0007442]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_10_loss_0.0007442.pth\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.94it/s, Loss=0.0033981]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_11_loss_0.0033981.pth\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.95it/s, Loss=0.0001568]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_12_loss_0.0001568.pth\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.95it/s, Loss=0.0004962]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_13_loss_0.0004962.pth\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.90it/s, Loss=0.0001391]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_14_loss_0.0001391.pth\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.94it/s, Loss=0.0003317]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_15_loss_0.0003317.pth\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.93it/s, Loss=0.0002175]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_16_loss_0.0002175.pth\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.93it/s, Loss=0.0001612]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_17_loss_0.0001612.pth\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.92it/s, Loss=0.0000705]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_18_loss_0.0000705.pth\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.94it/s, Loss=0.0001376]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_19_loss_0.0001376.pth\n",
            "\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 710/710 [00:47<00:00, 14.95it/s, Loss=0.0001460]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved: /storage/physics/phuftc/DDPM-HL-LHC/data/ML/second/checkpoint_epoch_20_loss_0.0001460.pth\n",
            "Finished training\n"
          ]
        }
      ],
      "source": [
        "# this one is to be passed into DataLoader for training\n",
        "train_batch_size = 100\n",
        "ng_for_dataloader = NGenForDataloader(NG_jet)\n",
        "# validation_jets = NGenForDataloader(NG_jet)\n",
        "# ng_for_dataloader = NGenForDataloader(NG_jet, batch_size=100)\n",
        "# We handle batching ourselves, by serving \"batch_size\" number of jets to train on each time\n",
        "dataloader = DataLoader(ng_for_dataloader, batch_size=train_batch_size, num_workers=2, shuffle = True, pin_memory = True)\n",
        "# validation_loader = DataLoader(ng_for_dataloader, batch_size=100, num_workers=2, shuffle = True, pin_memory = True)\n",
        "# dataloader = DataLoader(ng_for_dataloader, pin_memory = True)\n",
        "# print(dataloader)\n",
        "#print(ng_for_dataloader)\n",
        "# pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# show_tensor_images(pats, scale_factor=10)\n",
        "# print(tt.select_event(0).shape)\n",
        "save_dir = f\"{CWD}/data/ML/second\"\n",
        "num_epochs = 20\n",
        "\n",
        "\n",
        "# print(f\"{len(ng_for_dataloader)} jets, {pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)ng_for_dataloader.batch_size} batches, {num_epochs} epochs\")\n",
        "load_and_train(diffusion, dataloader, num_epochs=num_epochs, device=device, save_dir=save_dir, lr=1e-4)\n",
        "# trainer_params = {\n",
        "#         diffusion_model: PUDiffusion,\n",
        "#         dataloader: dataloader,\n",
        "#         train_batch_size: train_batch_size,\n",
        "#         gradient_accumulate_every: 1,\n",
        "#         augment_horizontal_flip : False,\n",
        "#         train_lr: 1e-4,\n",
        "#         train_num_steps : 200,\n",
        "#         ema_update_every : 10,\n",
        "#         ema_decay : 0.995,\n",
        "#         adam_betas: (0.9, 0.99),\n",
        "#         save_and_sample_every : 5,\n",
        "#         num_samples : 25,\n",
        "#         results_folder : './ML/results',\n",
        "#         amp : False,\n",
        "#         mixed_precision_type : 'fp32',\n",
        "#         split_batches : True,\n",
        "#         convert_image_to :None,\n",
        "#         calculate_fid : False,\n",
        "#         inception_block_idx : 2048,\n",
        "#         max_grad_norm : 1.0,\n",
        "#         num_fid_samples : 50000,\n",
        "#         save_best_and_latest_only : False\n",
        "# }\n",
        "# trainer = PUTrainer(**trainer_params)\n",
        "# trainer = PUTrainer(\n",
        "#   diffusion,\n",
        "#   dataloader,\n",
        "#   train_batch_size = train_batch_size,\n",
        "#   gradient_accumulate_every = 1,\n",
        "#   augment_horizontal_flip = True,\n",
        "#   train_lr = 1e-6,\n",
        "#   train_num_steps = 100,\n",
        "#   ema_update_every = 10,\n",
        "#   ema_decay = 0.995,\n",
        "#   adam_betas = (0.9, 0.99),\n",
        "#   save_and_sample_every = 20,\n",
        "#   num_samples = 100,\n",
        "#   results_folder = f'{CWD}/data/ML/results',\n",
        "#   amp = False,\n",
        "#   mixed_precision_type = 'fp32',\n",
        "#   split_batches = True,\n",
        "#   convert_image_to = None,\n",
        "#   calculate_fid = True,\n",
        "#   inception_block_idx = 2048,\n",
        "#   max_grad_norm = 1.,\n",
        "#   num_fid_samples = 50000,\n",
        "#   save_best_and_latest_only = False\n",
        "#   )\n",
        "# trainer.train()\n",
        "print(\"Finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aM0CGsUD1wKQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PUDiffusion(\n",
              "  (model): Unet(\n",
              "    (init_conv): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (time_mlp): Sequential(\n",
              "      (0): SinusoidalPosEmb()\n",
              "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (downs): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "          (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Identity()\n",
              "        )\n",
              "        (2): Attention(\n",
              "          (norm): RMSNorm()\n",
              "          (attend): Attend(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (ups): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Attention(\n",
              "          (norm): RMSNorm()\n",
              "          (attend): Attend(\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): Upsample(scale_factor=2.0, mode='nearest')\n",
              "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (3): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (mlp): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "          )\n",
              "          (block1): Block(\n",
              "            (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (block2): Block(\n",
              "            (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm): RMSNorm()\n",
              "            (act): SiLU()\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): LinearAttention(\n",
              "          (norm): RMSNorm()\n",
              "          (to_qkv): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (to_out): Sequential(\n",
              "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (mid_block1): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Identity()\n",
              "    )\n",
              "    (mid_attn): Attention(\n",
              "      (norm): RMSNorm()\n",
              "      (attend): Attend(\n",
              "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (to_qkv): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (to_out): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (mid_block2): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Identity()\n",
              "    )\n",
              "    (final_res_block): ResnetBlock(\n",
              "      (mlp): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "      )\n",
              "      (block1): Block(\n",
              "        (proj): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (block2): Block(\n",
              "        (proj): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (norm): RMSNorm()\n",
              "        (act): SiLU()\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (res_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "diffusion.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sampling loop time step: 100%|██████████| 200/200 [00:02<00:00, 96.76it/s]\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAC60lEQVR4Ae2bz4uOURiGv5fXryELFmp2drOaUqQkTcmUslQ2E8kspKxspWxssLKRrCZkIykkhRQ2RP4IIpE0YUNWcz0Wb6cz98M46Z7V1XPO/c4z13Pq9H3ffN3D0cLPwQUYHYdWQzuhV9AslIGThCegGWgZ1Ci4wexgbNAGswayeZ/BrMFuA0/4BA3BOorz0J+B2zzmELQF8ohRIYINiuKI2SAqRLBBURyx5g12t+h1CtoLbYfmoCko7pRH1M5CK6Cr0B7oKBT7vlK7AzVv0A0yKxFsUBRHzAZRIYINiuKIde/BTVBL4BFnp2GDNpg1kM37DP73Bvt72T8xnX/KE05BDyCfQVSIYIOiOGI2iAoRbFAUR6x5g903el0D1cJbNo5DT6D4jJ7SIExTnYTOQc0bdIPMSgQbFMURs0FUiGCDojhi/S5w8bCZyGnoJfQGug6dgAKegT+hAI84XGhkg5q3SNlguNDIBjVvkWreYP8uml003SVxDdoPTUCxSuk3OAPHfURp1LxBNxjD0sgGNW+RssFwoZENat4i1cf7SAeiOkDbqD2H4p2xH9Q2QqugI9AQ3KA4Bn2EPGJUiGCDojhiNogKEWxQFEeseYPdenr9AtVC3BpxG+wmfBgKC5eoBbwA48Z5TS2ylNoCN5idhw3aYNZANu8zmDXYzfKEy9AQTFO8D5XhCssz0BDEK5G1LH+APGJUiGCDojhiNogKEWxQFEeseYN9bYe19wd/+qh8f8S+eCVyIYpQbX8ElhrcYNa4Ddpg1kA27zOYNdhv5QlD7zyx+BdhZfHZHnFRT8WiDVZIKm6xwaKeikUbrJBU3NK8wf57sf+lWDxf/CXNG3SDxflVLNpghaTiFhss6qlYtMEKScUt/STL8f+7x6jNQfFdj3gN8ZnVi9AOKL67vpzaYyg+RZmndhMagzxiVIhgg6I4YjaIChFsUBRHrHmD3T56HYcuQf8emjfoBrOHxAZtMGsgm/cZzBr8BSDXPkl9jR5/AAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate samples\n",
        "# NG_jet.reset()\n",
        "# NG_pu.reset()\n",
        "sampled_images = diffusion.sample(batch_size=1)\n",
        "# print(sampled_images)\n",
        "show_tensor_images(sampled_images * NG_jet.max_energy, scale_factor=10)\n",
        "# next(NG_pu)\n",
        "# next(NG_jet)\n",
        "# sampled_images = diffusion.sample(batch_size=1)\n",
        "# show_tensor_images(sampled_images, scale_factor=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70901\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiit3/AIRv/p7/APIf/wBesm9tvsl28G/ftx82MZyM1BRRRRRRRRRRRRRRRRRXWf2zp/8Az8f+ON/hXPanNHcahLLE25Gxg4x2FVKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK9P8ACXwf/wCEp8MWes/279l+07/3P2Tft2uy9d4z93PTvWL8QPh//wAIL/Z3/Ez+3fbPN/5d/K2bNv8AtNnO79K4qiiiiiiiiiiiiiiiiivd/h38RPCuheBNN03UtU8i7h83zI/s8rYzK7DlVI6Ed65f4weLdD8U/wBjf2Nffavs3n+b+6dNu7y8feUZ+6enpXmFLg0/yJP7v60xlKnBGDSUUUUUUUUUUUUUUUUo606rtU5/9c1R0UUUUUUUUUUUUUUU6P74qevaa8p8Y/8AI1Xv/AP/AEBaw6KKKKKKKKKKKKKK6jwfpFjqv237bB5vl7NnzsuM7s9CPQVqeIfD2l6foVzdWtr5cybdreYxxlgDwTjoa4XzG9a2v+Ex17/n/wD/ACCn/wATWVeXlxqF291dSeZM+NzYAzgYHA46CoKKKKKKKKKKKKKKK2vD/iD+wvtH+i+f523/AJabcYz7H1q9q/jD+1dLmsvsPleZt+fzt2MMD02j0rl6KKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK7PQv+QNb/wDAv/QjWd4p/wCXT/gf/stc7RRX0v8ACn/kmukf9tv/AEc9cZ8ef+Zf/wC3n/2lXjdFFFFFFFFFFFFXYNWvbaFYYZ9sa5wNinvnuKiur+5vdn2iTfszt+UDGfp9Kr0UV02lfEDxRoemQ6dp2p+TaQ7tkf2eJsZJY8spPUmqWveK9b8T/Z/7YvftP2fd5X7pE27sZ+6Bn7o6+lY1FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAABL0lEQVR4Ae3ZOwrCUBQEUH/YuRN7C1tXYuliLF2NrYW9C3EJIsKFgM3L+A0n1SQwSd7hQkgyGtkIECBAgAABAgQIECDwPYHxqy+9rhOeKiVhkpQ/0XWDqTJBgqlA2jeDqaA+AQIECBAgQIAAAQIECBAgQIAAgf8V6PGf5FKrXVZ6X/ABM7UlSDAVSPtmcPCCPZ4kz0ymdXBR6VopCWYw0bt3CRJMBdK+GRy84Cxd4aM/r9NsK+0rJcEMJnr3LkGCqUDaN4ODF4zeSXbFc6j06mAGU1GCBFOBtG8GBy+YLlCfAAECBAgQIECAAAECiUCPr1ubut6x0rNwroOrSu3Be3G7WbdBsOvRvkew3azbINj1aN/7ecH2JWkQIECAAAECBAgQIECAAAECBAgQ+BWBGycMB2zzraOaAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigAk4FO2N6Um0+lJ0oooooooooooooooopyffFTVHTG+9SUUUUUUUUUUUUUUUVNa/8fKfj/KtKqNV5P9YabRRRRRRRRRRRRRRRSo7IwZTgipftU39/9BUfmP6/pSEknJ60lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA0ElEQVR4Ae3VuwqAMAwF0CoOjv7/Zzo7OAiRglMiPuB0Sgs3tKeFtmYQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIPAvgeGe7c7RZruoYilRjInMoxEbrHITJFgVqOa9wargVG1w5Jdos0Z1/imxlChccQKtixDsOBITggm0LkKw40hMPi+YOJMIAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKvCuyf4AQodGH7UgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiinBGIyBxSMpXqKSiiiiiiiiiiiiiiiiip4/9WKbN/DUVFFFFFFFFFFFFFFFFFFFFFFFPj7049KbTKKKKKKKKKKKKKKKKKUEjpRuPrRk0lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf/Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAAy0lEQVR4Ae3VMQqAMAwAwOrg4P/f6ia4ZcoSY6HCdQohacO10DEsAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgT+JbDVxz2j5YpoXrDP2/qbnQ3YdSRIsCvQ7fcGu4IvfpL6kUe03EkUqTRwxSlLIUmwgJWWEkxZCkmCBay0dHnBdGpJAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQWFngAAiQDJxK7K9IAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiinKjPnaM4oMbqMkcU2iiiiiiiiiiiiiiiiiprf+L8Kkm/1RqrRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRSjk07aPSmUUUUUUUUUUUUUUUUUUA4NO3e1Noooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooor//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAAy0lEQVR4Ae3XMQqAMAwAwCqu/v+hrg7dMiZIKihcp0CSNhwd2jEsAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgT+JbB1xj2j+YpodbCv3nD1fgbsihIk2BXo9ruDXUH9BAgQIECgJ9D6kzw/+oiWO6I88FjIfeoswdooryCY+9RZgrVRXvF5wXx8WQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBB4Q2ACUjsCJ5swXicAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiilpKKKKKKKKKKKKKKKKKKVfvU+o6KKKKKKKKKKKKKKKKKUdadTKKKKKKKKKKKKKKKKUDIoIxQDzS5ptFFFFFFFFFFFFFFFOBGKQnNJS0lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFS7F9KjYYYgUlFFFFFFFFFFFFFFFA5NLt96mqJ/vmm0UUUUUUUUUUUUUUUo60uRT96+tRscsSKSiiiiiiiiiiiiiiiiiiiiiiilpDRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX//2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAABMUlEQVR4Ae3Z7QqCQBAF0L6g93/coCgi2EEIYr0brHL6NUp3WI8Dsno4+BEgQIAAAQIECBAgQIAAAQIECBDYqsBxzMKrzXNMw9bl1KpJCwtMbwxBgqlAmjeDqeAlbfDJn1ube6vGFG5x6kiQYCqQ5s3g7gWjJ8n/diLlbgbLYl1FcJ1bpQiWxbqK4Dq3Sk0vWA+DWvSPqvYfj/bPauPtVkOZpJh+Bi0wnRSCBFMBeQIECBAgQIAAAQIECGxZoF5LDbqKa+tza1VS2Lgneu8sQYKpQJo3g7sXjL64l059O6lzYyozmDoSJJgKpHkzuHvB4XuSfrFawrev9WawX3SZILj06D8i2G+2TBBcevQfTS/Yf0kSBAgQIECAAAECBAgQIECAAAECBAgQIECAAIF9CLwAZKoHkSRGLYwAAAAASUVORK5CYII=",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigUtJRRRRRRRRRRRRRRRRRSig9KSiiiiiiiiiiiiiiiiigHFLmkooooooooooooooooooooooooooooooooooooooooooooooooooooop8P+tFWqpUUUUUUUUUUUUUUUUUUqkhgR1qTzH9f0qKiiiiiiiiiiiiiiiiigcGl3e1JRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRX/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA50lEQVR4Ae3Yuw6CQBRFUXy0/v+HWhvthhayR0J0Ud2CM481lxBYFhcBAgQIECBAgAABAgQIECBAgAABAgQIECBA4D8FLkdse53kvXu66+7EwQELrOAECVaBmteDVfBeB9iS9ybZovStezwkVZYgwSpQ86fvwbpBeQIECBAgQIAAAQIECBSB9cdTGWV5jPRzVHOK03/VWWA9aIIEq0DN68EqOOlNchvreI1qTuGIqyNBglWg5vXgzwvWDcoTIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAwH6BD5obBFCdYRlkAAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiijBooooooooooooooooooop1NPWiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiilwaNjHtQVI60lFFFFFFFFFFFFFFFFPp6/dpknamUUUUUUUUUUUUUUUUUUUUUUVs2H/HlH+P8zVfVf+WX4/0rOoooooooooooooooooqeO7niQIj4UdBgU2aeSbHmNux04AqKiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiv/2Q==",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA/ElEQVR4Ae3ZTQ6CMBAGUPw5ifc/k/dwo+6GzK4zDYj6WH1N+GjzaNIElsVFgAABAgQIECBAgAABAgQIECBA4FsFTqMLP8eNz0h7hHXePWZrzGGBDbRUIZg4GgOCDbRUIZg4DAgQIECAAAECBAgQIECAAAECBAj8lcDwf5JLsFwjPSJtF3zAnLUlSHBWYLZvD/684PBJUpe4ReUeqR7swbpZbhDMHvURwbpZbhDMHvXR4QU3PEnWR7/qcNE4vKAFxrtqBoJNuKgRDIpmINiEUyNAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgAABAgQIECBAgACBDwq8AU/MBE1e9e58AAAAAElFTkSuQmCC",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCACgAKABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiigAk4FO2N6Um0+lJ0oooooooooooooooopyffFTVHTG+9SUUUUUUUUUUUUUUUVNa/8fKfj/KtKqNV5P9YabRRRRRRRRRRRRRRRSo7IwZTgipftU39/9BUfmP6/pSEknJ60lFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFf//Z",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAAAAACupDjxAAAA0ElEQVR4Ae3VuwqAMAwF0CoOjv7/Zzo7OAiRglMiPuB0Sgs3tKeFtmYQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIECAAAECBAgQIPAvgeGe7c7RZruoYilRjInMoxEbrHITJFgVqOa9wargVG1w5Jdos0Z1/imxlChccQKtixDsOBITggm0LkKw40hMPi+YOJMIAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQIECBAgQIAAAQKvCuyf4AQodGH7UgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=160x160>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(len(ng_for_dataloader))\n",
        "pats = [ng_for_dataloader[0],ng_for_dataloader[1],ng_for_dataloader[2],ng_for_dataloader[3],ng_for_dataloader[4],ng_for_dataloader[5], ng_for_dataloader[-1]]\n",
        "# print(ng_for_dataloader[0])\n",
        "ng_for_dataloader_unscaled = ng_for_dataloader[0] * NG_jet.max_energy\n",
        "show_tensor_images(ng_for_dataloader_unscaled, scale_factor=10)\n",
        "show_tensor_images(pats, scale_factor=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pvdsNEqC_RoD"
      },
      "outputs": [],
      "source": [
        "# class PileupDiffusion(GaussianDiffusion):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         model,\n",
        "#         *,\n",
        "#         image_size,\n",
        "#         timesteps = 1000,\n",
        "#         sampling_timesteps = None,\n",
        "#         objective = 'pred_v',\n",
        "#         beta_schedule = 'sigmoid',\n",
        "#         schedule_fn_kwargs = dict(),\n",
        "#         ddim_sampling_eta = 0.,\n",
        "#         auto_normalize = True,\n",
        "#         offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
        "#         min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n",
        "#         min_snr_gamma = 5,\n",
        "#         immiscible = False\n",
        "#     ):\n",
        "#         super().__init__()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f901c8354c4a168759d830710b5bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12c9b8cba5bb44109c3d460a8bb9eaec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170d582725ff42ee92bf089a5e99db82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f594a87e924046bbb046ac798b4be159",
            "placeholder": "​",
            "style": "IPY_MODEL_c3b46ca3a3984adcb72c77ac9790d081",
            "value": "sampling loop time step: 100%"
          }
        },
        "18a78d0165cc4d31ab6f914477a1e38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ae5e9f79e548518f1103d6b2fcc020",
            "placeholder": "​",
            "style": "IPY_MODEL_69645c7b3d44463db12630c2b28d6311",
            "value": " 1000/1000 [01:18&lt;00:00, 12.90it/s]"
          }
        },
        "1b4c24252e69464b8b55219b09f5719f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22562c7b650b460b90d3986d1cf2dbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bbf4414cd6c4b678c117e991ef3500a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea47496db3c40f38a350ba21365d3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3e74f819dbd4b81bf1c7ba0f9c15670",
            "placeholder": "​",
            "style": "IPY_MODEL_22562c7b650b460b90d3986d1cf2dbdd",
            "value": "sampling loop time step: 100%"
          }
        },
        "40f855e865d141ffbecdcc909feb5881": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc3db99c8434e708ab613a8de71da86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c517885cfa4c5db1b350775e481a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d9a21a1d7ac4c1082d537a32ab78669",
              "IPY_MODEL_f81e3ba126724d7f95a5140b89c127a6",
              "IPY_MODEL_64899dafc9b748dd985dd1800f5a18c9"
            ],
            "layout": "IPY_MODEL_677cf3826fe5477c9c13f2458ae18788"
          }
        },
        "58ae5e9f79e548518f1103d6b2fcc020": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5904f0aeba2648f0903cb0738963038a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b79306793a24f4c8bd46c78cb515732": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61cf806af9cd4a8aab4e64e6bfa81fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_170d582725ff42ee92bf089a5e99db82",
              "IPY_MODEL_c71eb60fe15f4cb7b9182a4b5a677068",
              "IPY_MODEL_98dc9e829f4a4292aac01ab3990be72b"
            ],
            "layout": "IPY_MODEL_5b79306793a24f4c8bd46c78cb515732"
          }
        },
        "627d07804d5e4d99ba2915a52a006796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64899dafc9b748dd985dd1800f5a18c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc34e955ddb9409db13400a9fe1d4af0",
            "placeholder": "​",
            "style": "IPY_MODEL_fd30649a40ff4132b3365373e551173a",
            "value": " 1000/1000 [01:18&lt;00:00, 12.99it/s]"
          }
        },
        "677cf3826fe5477c9c13f2458ae18788": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69645c7b3d44463db12630c2b28d6311": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7341f14b06d74fc7bf57ff66782d8014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75e385e73db94654a515f4ba28586269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ea47496db3c40f38a350ba21365d3ca",
              "IPY_MODEL_e843a479201049b89afe61c4eeee7e22",
              "IPY_MODEL_d9f998afc18a4eadb5bd69b98ee08c9f"
            ],
            "layout": "IPY_MODEL_b53688b74fc64b42b8569d1cc52bba8e"
          }
        },
        "7bfdb8a6411c46deb96094d5de110b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_902a5564e03748fe93b872cdaa8f6d9d",
              "IPY_MODEL_ffbf32f91d7a4ec39df292c13e62ccd6",
              "IPY_MODEL_18a78d0165cc4d31ab6f914477a1e38e"
            ],
            "layout": "IPY_MODEL_fefd1abd335e4775a1a267f0008151c6"
          }
        },
        "902a5564e03748fe93b872cdaa8f6d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c9b8cba5bb44109c3d460a8bb9eaec",
            "placeholder": "​",
            "style": "IPY_MODEL_627d07804d5e4d99ba2915a52a006796",
            "value": "sampling loop time step: 100%"
          }
        },
        "98dc9e829f4a4292aac01ab3990be72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc43b41d36bf4fd1b3b576e06d7880e4",
            "placeholder": "​",
            "style": "IPY_MODEL_7341f14b06d74fc7bf57ff66782d8014",
            "value": " 250/250 [00:19&lt;00:00, 12.68it/s]"
          }
        },
        "9b5861f6266849eeaad610b5f23a6bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d9a21a1d7ac4c1082d537a32ab78669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc465482e8443c495f2156aa5fc4ceb",
            "placeholder": "​",
            "style": "IPY_MODEL_4fc3db99c8434e708ab613a8de71da86",
            "value": "sampling loop time step: 100%"
          }
        },
        "aa1292a549ef4c2ca499d83d9e569381": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc465482e8443c495f2156aa5fc4ceb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53688b74fc64b42b8569d1cc52bba8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b46ca3a3984adcb72c77ac9790d081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3e74f819dbd4b81bf1c7ba0f9c15670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c041868d084fd2805330058b588fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c71eb60fe15f4cb7b9182a4b5a677068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bbf4414cd6c4b678c117e991ef3500a",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01f901c8354c4a168759d830710b5bac",
            "value": 250
          }
        },
        "cc34e955ddb9409db13400a9fe1d4af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc43b41d36bf4fd1b3b576e06d7880e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f998afc18a4eadb5bd69b98ee08c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa1292a549ef4c2ca499d83d9e569381",
            "placeholder": "​",
            "style": "IPY_MODEL_1b4c24252e69464b8b55219b09f5719f",
            "value": " 250/250 [00:39&lt;00:00,  6.57it/s]"
          }
        },
        "dfcebb41e86c4cfb993522f80b7184ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e542a5c11e9648629cd75ceab7caaf3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e843a479201049b89afe61c4eeee7e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f855e865d141ffbecdcc909feb5881",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e542a5c11e9648629cd75ceab7caaf3d",
            "value": 250
          }
        },
        "f594a87e924046bbb046ac798b4be159": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81e3ba126724d7f95a5140b89c127a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4c041868d084fd2805330058b588fa6",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfcebb41e86c4cfb993522f80b7184ba",
            "value": 1000
          }
        },
        "fd30649a40ff4132b3365373e551173a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fefd1abd335e4775a1a267f0008151c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffbf32f91d7a4ec39df292c13e62ccd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5904f0aeba2648f0903cb0738963038a",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b5861f6266849eeaad610b5f23a6bb8",
            "value": 1000
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
